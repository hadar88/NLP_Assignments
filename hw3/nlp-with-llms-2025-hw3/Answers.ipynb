{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debb5812",
   "metadata": {},
   "source": [
    "# Part 0\n",
    "\n",
    "## Summary of Key Motivations and Contributions\n",
    "\n",
    "* Most QA systems today answer literally, providing facts but not necessarily what's most helpful in conversation. For example, if you ask \"Is there water on Mars?\" a literal answer would be \"Yes\", but a friendly answer would be \"Yes, but only in the form of ice caps near its poles.\".\n",
    "\n",
    "* Recent open-domain question answering (QA) datasets still fall short at two crucial desiderata.\n",
    "    1. Existing QA datasets only test whether systems can find literal answers, not whether they understand what you really want to know\n",
    "    2. Most of these datasets are crowd-sourced, which leaves them vulnerable to\n",
    "    the problem of incentive misalignment between annotators and potential users.\n",
    "\n",
    "* The contributions of the paper are:\n",
    "    1. PRAGMATICQA dataset: They created the first conversational QA dataset that includes pragmatic answers (going beyond literal responses) and developed new metrics to measure how well AI systems can do pragmatic reasoning.\n",
    "    2. Improved data collection: They designed a crowdsourcing method that fixes the incentive misalignment problem - where crowdworkers aren't motivated like real users would be. This produced more realistic, high-quality, and diverse conversation data.\n",
    "    3. Current system analysis: They tested the dataset and showed it poses unique and important challenges that today's conversational QA systems can't handle well.\n",
    "\n",
    "## Why this dataset challenging?\n",
    "\n",
    "* PragmatiCQA is hard for NLP models because it tests whether models can have actual conversations, not just answer questions robotically. The dataset focuses on two key human conversational skills: anticipating what someone really wants to know (like explaining that Mars has water \"but only as ice at the poles\" instead of just saying \"yes\") and offering helpful context that keeps the conversation going (mentioning that 23 places in our solar system have water). What makes this super challenging is that AI has to guess what the person already knows, predict their likely follow-up questions, decide what extra info would actually be interesting - basically doing the kind of helpful, context responding that humans do but current NLP systems are terrible at, missing over 90% of the useful extra information humans naturally provide.\n",
    "\n",
    "* It targets also few pragmatic phenomena\n",
    "    - **Implicature**: Providing information that is implied or suggested, not just what is explicitly asked.\n",
    "    - **Presupposition**: Recognizing and addressing background assumptions in the question.\n",
    "    - **Relevance**: Selecting information that is most useful or interesting to the user, not just factually correct.\n",
    "    - **Anticipation of follow-up**: Including information that preempts likely follow-up questions, making the conversation smoother and more natural.\n",
    "    - **Disambiguation**: Clarifying ambiguous questions by inferring the user's intent.\n",
    "\n",
    "## Dataset samples\n",
    "\n",
    "1. **The Legend of Zelda**\n",
    "    - **Question**: \"What year did the Legend of Zelda come out?\"\n",
    "    - **Literal Answer**: \"1986\"\n",
    "    - **Pragmatic Answer**: \"The Legend of Zelda came out as early as 1986 for the Famicom in Japan, and was later released in the western world, including Europe and the US in 1987. Would you like to know about the story?\"\n",
    "    - **Enrichment**: The pragmatic answer not only provides the year but also adds information about the release in different places in the world and also anticipates a follow-up question and asking if the user wants to know more about the story.\n",
    "2. **Po**\n",
    "    - **Question**: \"who is the main character ?\"\n",
    "    - **Literal Answer**: \"Po\"\n",
    "    - **Pragmatic Answer**: \"The main character of the movie is a panda named Po. He is also known as the Dragon Warrior. The Dragon Warrior is a prodigy of legend who is described in the first film to know the secret behind the Dragon Scroll. The Dragon Scroll is a legendary scroll that contains the secret to \\\"limitless power\\\" and had been written by Oogway.\"\n",
    "    - **Enrichment**: The pragmatic answer provides the name of the main character, Po, but also adds context about his role as the Dragon Warrior and the significance of the Dragon Scroll, which is relevant to understanding the character's importance in the story.\n",
    "3. **Snoop Dogg**\n",
    "    - **Question**: \"Who is Snoop Dogg?\"\n",
    "    - **Literal Answer**: \"Snoop Dogg is an American rapper.\"\n",
    "    - **Pragmatic Answer**: \"Snoop Dogg is an American rapper who has sold over 23 millions albums nationally and over 35 million albums worldwide. Snoop Dogg sings and writes his own songs and produces records and even acts in films and television with his own personality.\"\n",
    "    - **Enrichment**: The pragmatic answer provides the basic fact that Snoop Dogg is a rapper but also adds significant context about his achievements, including album sales and his multifaceted career in music and entertainment, which makes the answer more informative and engaging.\n",
    "4. **Scuderia Ferrari**\n",
    "    - **Question**: \"Who is Scuderia Ferrari?\"\n",
    "    - **Literal Answer**: \"Scuderia Ferrari is the racing division of Ferrari.\"\n",
    "    - **Pragmatic Answer**: \"Scuderia Ferrari is the racing division of Ferrari, and they are one of the most successsful teams in F1. They've won the drivers' title 15 times since the 1950s, and started making their own cars since 1947.\"\n",
    "    - **Enrichment**: The pragmatic answer not only identifies Scuderia Ferrari as the racing division of Ferrari but also provides context about their success in Formula 1, including their championship wins and history, which adds depth to the answer.\n",
    "\n",
    "5. **King Julien**\n",
    "    - **Question**: \"Who is King Julien?\"\n",
    "    - **Literal Answer**: \"King Julien is a character in the Madagascar franchise.\"\n",
    "    - **Pragmatic Answer**: \"King Julien XIII is one of the main characters in the Madagascar franchise. He is the King of the Kingdom of Madagascar, ruling the lemur kingdom since his uncle, King Julien XII abdicated in King Me.\",\n",
    "    - **Enrichment**: The pragmatic answer provides the basic fact that King Julien is a character in the Madagascar franchise but also adds context about his role as the king of the lemur kingdom and his lineage, which enriches the understanding of his character within the story."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780c4d9",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad4951a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dspy\n",
    "import json\n",
    "import types\n",
    "import hashlib\n",
    "from dspy.evaluate import SemanticF1\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load environment variables and set up the language model\n",
    "load_dotenv(\"grok_key.ini\")\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'], max_tokens=6000, temperature=0.1, top_p=0.9)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Set up the retriever from rag.ipynb\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "# Load the pre-traind QA model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "def list_folders(directory=\"../PragmatiCQA-sources\"):\n",
    "    return [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "folders = list_folders()\n",
    "\n",
    "# Traverse a directory and read html files - extract text from the html files\n",
    "def read_html_files(dir_name, directory=\"../PragmatiCQA-sources\"):\n",
    "    texts = []\n",
    "    for filename in os.listdir(os.path.join(directory, dir_name)):\n",
    "        if filename.endswith(\".html\"):\n",
    "            with open(os.path.join(directory, dir_name, filename), 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'html.parser')\n",
    "                texts.append(soup.get_text())\n",
    "    return texts\n",
    "\n",
    "# Create retriever for a specific topic\n",
    "def make_search(topic):\n",
    "    corpus = read_html_files(topic)\n",
    "    max_characters = 10000 \n",
    "    topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "    return dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve, brute_force_threshold=max_characters)\n",
    "\n",
    "# Load PragmatiCQA dataset\n",
    "def read_data(filename, dataset_dir=\"../PragmatiCQA/data\"):\n",
    "    corpus = []\n",
    "    with open(os.path.join(dataset_dir, filename), 'r') as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    return corpus\n",
    "\n",
    "class TraditionalQA:\n",
    "    def __init__(self):\n",
    "        self.qa_pipeline = qa_pipeline\n",
    "\n",
    "    def answer_from_context(self, question, context):\n",
    "        result = self.qa_pipeline(question=question, context=context)\n",
    "        return result['answer']\n",
    "    \n",
    "    def answer_from_retriever(self, question, search):\n",
    "        passages = search(question).passages\n",
    "        context = \" \".join(passages)\n",
    "        return self.answer_from_context(question, context)\n",
    "    \n",
    "# Load the datasets\n",
    "test_data = read_data(\"test.jsonl\")\n",
    "val_data = read_data(\"val.jsonl\")\n",
    "\n",
    "# Initialize the model\n",
    "traditional_qa = TraditionalQA()\n",
    "\n",
    "# Function to evaluate a configuration\n",
    "def evaluate_configuration(dataset, config):\n",
    "    examples = []\n",
    "    predictions = []\n",
    "\n",
    "    # Cache file for retrieved results\n",
    "    cache_file = \"retriever_cache.json\"\n",
    "    # Load cache if exists\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            retriever_cache = json.load(f)\n",
    "    else:\n",
    "        retriever_cache = {}\n",
    "\n",
    "    def get_cache_key(topic, question):\n",
    "        key_str = f\"{topic}|{question}\"\n",
    "        return hashlib.md5(key_str.encode()).hexdigest()\n",
    "\n",
    "    for conversation in dataset:\n",
    "        topic = conversation['topic']\n",
    "        if topic not in folders:\n",
    "            continue\n",
    "        first_qa = conversation['qas'][0]\n",
    "\n",
    "        question = first_qa['q']\n",
    "        gold_answer = first_qa['a']\n",
    "\n",
    "        if config == \"Literal\":\n",
    "            lit_spans = [l['text'] for l in first_qa['a_meta']['literal_obj']]\n",
    "            context = ' '.join(lit_spans)\n",
    "            pred_answer = traditional_qa.answer_from_context(question, context)\n",
    "        elif config == \"Pragmatic\":\n",
    "            prag_spans = [l['text'] for l in first_qa['a_meta']['pragmatic_obj']]\n",
    "            context = ' '.join(prag_spans)\n",
    "            pred_answer = traditional_qa.answer_from_context(question, context)\n",
    "        else:  # Retrieved]\n",
    "            cache_key = get_cache_key(topic, question)\n",
    "            if cache_key in retriever_cache:\n",
    "                pred_answer = retriever_cache[cache_key]\n",
    "            else:\n",
    "                search = make_search(topic)\n",
    "                passages = search(question).passages\n",
    "                context = \" \".join(passages)\n",
    "                pred_answer = traditional_qa.answer_from_context(question, context)\n",
    "                retriever_cache[cache_key] = pred_answer\n",
    "                \n",
    "                with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(retriever_cache, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        example = dspy.Example(question=question, response=gold_answer)\n",
    "        pred = dspy.Example(question=question, response=pred_answer)\n",
    "\n",
    "        examples.append(example)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    return examples, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93f044a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the model on the PRAGMATICQA test set...\n",
      "Evaluating configuration: Literal\n",
      "Evaluating configuration: Pragmatic\n",
      "Evaluating configuration: Retrieved\n",
      "\n",
      "Evaluating predictions using SemanticF1...\n",
      "\n",
      "Test set results:\n",
      "Configuration | F1 Score | Count\n",
      "-----------------------------------\n",
      "Literal       | 0.4274   | 120\n",
      "Pragmatic     | 0.3544   | 120\n",
      "Retrieved     | 0.1223   | 120\n"
     ]
    }
   ],
   "source": [
    "print(\"Running the model on the PRAGMATICQA test set...\")\n",
    "test_predictions = {}\n",
    "for config in [\"Literal\", \"Pragmatic\", \"Retrieved\"]:\n",
    "    print(f\"Evaluating configuration: {config}\")\n",
    "    examples, predictions = evaluate_configuration(test_data, config)\n",
    "    test_predictions[config] = {\n",
    "        'examples': examples,\n",
    "        'predictions': predictions,\n",
    "    }\n",
    "\n",
    "# Evaluate the predictions using SemanticF1\n",
    "print(\"\\nEvaluating predictions using SemanticF1...\")\n",
    "metric = SemanticF1(decompositional=True)\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for config in [\"Literal\", \"Pragmatic\", \"Retrieved\"]:\n",
    "    examples = test_predictions[config]['examples']\n",
    "    predictions = test_predictions[config]['predictions']\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for example, prediction in zip(examples, predictions):\n",
    "        score = metric(example, prediction)\n",
    "        f1_scores.append(score)\n",
    "\n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    test_results[config] = {\n",
    "        'f1': avg_f1, \n",
    "        'count': len(examples)\n",
    "    }\n",
    "\n",
    "# Display test results table\n",
    "print(\"\\nTest set results:\")\n",
    "print(\"Configuration | F1 Score | Count\")\n",
    "print(\"-\" * 35)\n",
    "for config, results in test_results.items():\n",
    "    print(f\"{config:<13} | {results['f1']:.4f}   | {results['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43925c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the model on the PRAGMATICQA val set...\n",
      "Evaluating configuration: Literal\n",
      "Evaluating configuration: Pragmatic\n",
      "Evaluating configuration: Retrieved\n",
      "\n",
      "Evaluating predictions using SemanticF1...\n",
      "Evaluating configuration: Literal\n",
      "Evaluating configuration: Pragmatic\n",
      "Evaluating configuration: Retrieved\n",
      "\n",
      "Validation set results:\n",
      "Configuration | Precision | Recall | F1 Score | Count\n",
      "------------------------------------------------------------\n",
      "Literal       | 0.8617   | 0.2738  | 0.4003   | 135\n",
      "Pragmatic     | 0.8037   | 0.2585  | 0.3714   | 135\n",
      "Retrieved     | 0.0914   | 0.0324  | 0.0394   | 135\n",
      "\n",
      "Analysis:\n",
      "Best configuration on test set: Literal with F1 score 0.4274\n",
      "Best configuration on validation set: Literal with F1 score 0.4003\n",
      "Total Cost: 0.13 usd\n"
     ]
    }
   ],
   "source": [
    "print(\"Running the model on the PRAGMATICQA val set...\")\n",
    "val_predictions = {}\n",
    "for config in [\"Literal\", \"Pragmatic\", \"Retrieved\"]:\n",
    "    print(f\"Evaluating configuration: {config}\")\n",
    "    examples, predictions = evaluate_configuration(val_data, config)\n",
    "    val_predictions[config] = {\n",
    "        'examples': examples,\n",
    "        'predictions': predictions,\n",
    "    }\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "# Wrap the forward method to extract scores manually\n",
    "def patched_forward(self, example, pred, trace=None):\n",
    "    scores = self.module(\n",
    "        question=example.question,\n",
    "        ground_truth=example.response,\n",
    "        system_response=pred.response\n",
    "    )\n",
    "    f1 = f1_score(scores.precision, scores.recall)\n",
    "    return {\n",
    "        \"precision\": scores.precision,\n",
    "        \"recall\": scores.recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Evaluate the predictions using SemanticF1\n",
    "print(\"\\nEvaluating predictions using SemanticF1...\")\n",
    "metric = SemanticF1(decompositional=True)\n",
    "metric.forward = types.MethodType(patched_forward, metric)\n",
    "\n",
    "val_results = {}\n",
    "\n",
    "score_cache_file = \"val_score_cache.json\"\n",
    "if os.path.exists(score_cache_file):\n",
    "    with open(score_cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        score_cache = json.load(f)\n",
    "else:\n",
    "    score_cache = {}\n",
    "\n",
    "for config in [\"Literal\", \"Pragmatic\", \"Retrieved\"]:\n",
    "    print(f\"Evaluating configuration: {config}\")\n",
    "    examples = val_predictions[config]['examples']\n",
    "    predictions = val_predictions[config]['predictions']\n",
    "\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for example, prediction in zip(examples, predictions):\n",
    "        # Create a unique key for each example-prediction pair\n",
    "        cache_key = f\"{config}|{example.question}|{prediction.response}\"\n",
    "\n",
    "        if cache_key not in score_cache:\n",
    "            result = metric(example, prediction)\n",
    "            score = [result[\"precision\"], result[\"recall\"], result[\"f1\"]]\n",
    "            score_cache[cache_key] = score\n",
    "            # Save the scores to a JSON file\n",
    "            with open(score_cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(score_cache, f, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            score = score_cache[cache_key]\n",
    "\n",
    "        # Extract precision, recall, F1 if available in decomposed format\n",
    "        precision = score[0]\n",
    "        recall = score[1]\n",
    "        f1 = score[2]\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "    avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    val_results[config] = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1': avg_f1,\n",
    "        'count': len(examples)\n",
    "    }\n",
    "\n",
    "# Display validation results table\n",
    "print(\"\\nValidation set results:\")\n",
    "print(\"Configuration | Precision | Recall | F1 Score | Count\")\n",
    "print(\"-\" * 60)\n",
    "for config, results in val_results.items():\n",
    "    print(f\"{config:<13} | {results['precision']:.4f}   | {results['recall']:.4f}  | {results['f1']:.4f}   | {results['count']}\")\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "best_test_config = max(test_results, key=lambda k: test_results[k]['f1'])\n",
    "best_val_config = max(val_results, key=lambda k: val_results[k]['f1'])\n",
    "\n",
    "print(f\"Best configuration on test set: {best_test_config} with F1 score {test_results[best_test_config]['f1']:.4f}\")\n",
    "print(f\"Best configuration on validation set: {best_val_config} with F1 score {val_results[best_val_config]['f1']:.4f}\")\n",
    "\n",
    "cost = sum([x['cost'] for x in lm.history if x['cost'] is not None])\n",
    "print(f\"Total Cost: {cost:.2f} usd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
