{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debb5812",
   "metadata": {},
   "source": [
    "# Part 0\n",
    "\n",
    "## Summary of Key Motivations and Contributions\n",
    "\n",
    "* Most QA systems today answer literally, providing facts but not necessarily what's most helpful in conversation. For example, if you ask \"Is there water on Mars?\" a literal answer would be \"Yes\", but a friendly answer would be \"Yes, but only in the form of ice caps near its poles.\".\n",
    "\n",
    "* Recent open-domain question answering (QA) datasets still fall short at two crucial desiderata.\n",
    "    1. Existing QA datasets only test whether systems can find literal answers, not whether they understand what you really want to know\n",
    "    2. Most of these datasets are crowd-sourced, which leaves them vulnerable to\n",
    "    the problem of incentive misalignment between annotators and potential users.\n",
    "\n",
    "* The contributions of the paper are:\n",
    "    1. PRAGMATICQA dataset: They created the first conversational QA dataset that includes pragmatic answers (going beyond literal responses) and developed new metrics to measure how well AI systems can do pragmatic reasoning.\n",
    "    2. Improved data collection: They designed a crowdsourcing method that fixes the incentive misalignment problem - where crowdworkers aren't motivated like real users would be. This produced more realistic, high-quality, and diverse conversation data.\n",
    "    3. Current system analysis: They tested the dataset and showed it poses unique and important challenges that today's conversational QA systems can't handle well.\n",
    "\n",
    "## Why this dataset challenging?\n",
    "\n",
    "* PragmatiCQA is hard for NLP models because it tests whether models can have actual conversations, not just answer questions robotically. The dataset focuses on two key human conversational skills: anticipating what someone really wants to know (like explaining that Mars has water \"but only as ice at the poles\" instead of just saying \"yes\") and offering helpful context that keeps the conversation going (mentioning that 23 places in our solar system have water). What makes this super challenging is that AI has to guess what the person already knows, predict their likely follow-up questions, decide what extra info would actually be interesting - basically doing the kind of helpful, context responding that humans do but current NLP systems are terrible at, missing over 90% of the useful extra information humans naturally provide.\n",
    "\n",
    "* It targets also few pragmatic phenomena\n",
    "    - **Implicature**: Providing information that is implied or suggested, not just what is explicitly asked.\n",
    "    - **Presupposition**: Recognizing and addressing background assumptions in the question.\n",
    "    - **Relevance**: Selecting information that is most useful or interesting to the user, not just factually correct.\n",
    "    - **Anticipation of follow-up**: Including information that preempts likely follow-up questions, making the conversation smoother and more natural.\n",
    "    - **Disambiguation**: Clarifying ambiguous questions by inferring the user's intent.\n",
    "\n",
    "## Dataset samples\n",
    "\n",
    "1. **The Legend of Zelda**\n",
    "    - **Question**: \"What year did the Legend of Zelda come out?\"\n",
    "    - **Literal Answer**: \"1986\"\n",
    "    - **Pragmatic Answer**: \"The Legend of Zelda came out as early as 1986 for the Famicom in Japan, and was later released in the western world, including Europe and the US in 1987. Would you like to know about the story?\"\n",
    "    - **Enrichment**: The pragmatic answer not only provides the year but also adds information about the release in different places in the world and also anticipates a follow-up question and asking if the user wants to know more about the story.\n",
    "2. **Po**\n",
    "    - **Question**: \"who is the main character ?\"\n",
    "    - **Literal Answer**: \"Po\"\n",
    "    - **Pragmatic Answer**: \"The main character of the movie is a panda named Po. He is also known as the Dragon Warrior. The Dragon Warrior is a prodigy of legend who is described in the first film to know the secret behind the Dragon Scroll. The Dragon Scroll is a legendary scroll that contains the secret to \\\"limitless power\\\" and had been written by Oogway.\"\n",
    "    - **Enrichment**: The pragmatic answer provides the name of the main character, Po, but also adds context about his role as the Dragon Warrior and the significance of the Dragon Scroll, which is relevant to understanding the character's importance in the story.\n",
    "3. **Snoop Dogg**\n",
    "    - **Question**: \"Who is Snoop Dogg?\"\n",
    "    - **Literal Answer**: \"Snoop Dogg is an American rapper.\"\n",
    "    - **Pragmatic Answer**: \"Snoop Dogg is an American rapper who has sold over 23 millions albums nationally and over 35 million albums worldwide. Snoop Dogg sings and writes his own songs and produces records and even acts in films and television with his own personality.\"\n",
    "    - **Enrichment**: The pragmatic answer provides the basic fact that Snoop Dogg is a rapper but also adds significant context about his achievements, including album sales and his multifaceted career in music and entertainment, which makes the answer more informative and engaging.\n",
    "4. **Scuderia Ferrari**\n",
    "    - **Question**: \"Who is Scuderia Ferrari?\"\n",
    "    - **Literal Answer**: \"Scuderia Ferrari is the racing division of Ferrari.\"\n",
    "    - **Pragmatic Answer**: \"Scuderia Ferrari is the racing division of Ferrari, and they are one of the most successsful teams in F1. They've won the drivers' title 15 times since the 1950s, and started making their own cars since 1947.\"\n",
    "    - **Enrichment**: The pragmatic answer not only identifies Scuderia Ferrari as the racing division of Ferrari but also provides context about their success in Formula 1, including their championship wins and history, which adds depth to the answer.\n",
    "\n",
    "5. **King Julien**\n",
    "    - **Question**: \"Who is King Julien?\"\n",
    "    - **Literal Answer**: \"King Julien is a character in the Madagascar franchise.\"\n",
    "    - **Pragmatic Answer**: \"King Julien XIII is one of the main characters in the Madagascar franchise. He is the King of the Kingdom of Madagascar, ruling the lemur kingdom since his uncle, King Julien XII abdicated in King Me.\",\n",
    "    - **Enrichment**: The pragmatic answer provides the basic fact that King Julien is a character in the Madagascar franchise but also adds context about his role as the king of the lemur kingdom and his lineage, which enriches the understanding of his character within the story."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780c4d9",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4951a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dspy\n",
    "import json\n",
    "import types\n",
    "import hashlib\n",
    "from dspy.evaluate import SemanticF1\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load environment variables and set up the language model\n",
    "load_dotenv(\"grok_key.ini\")\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'], max_tokens=6000, temperature=0.1, top_p=0.9)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Set up the retriever from rag.ipynb\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "# Load the pre-traind QA model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "def list_folders(directory=\"../PragmatiCQA-sources\"):\n",
    "    return [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "folders = list_folders()\n",
    "\n",
    "# Traverse a directory and read html files - extract text from the html files\n",
    "def read_html_files(dir_name, directory=\"../PragmatiCQA-sources\"):\n",
    "    texts = []\n",
    "    for filename in os.listdir(os.path.join(directory, dir_name)):\n",
    "        if filename.endswith(\".html\"):\n",
    "            with open(os.path.join(directory, dir_name, filename), 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'html.parser')\n",
    "                texts.append(soup.get_text())\n",
    "    return texts\n",
    "\n",
    "# Create retriever for a specific topic\n",
    "def make_search(topic):\n",
    "    corpus = read_html_files(topic)\n",
    "    max_characters = 10000 \n",
    "    topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "    return dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve, brute_force_threshold=max_characters)\n",
    "\n",
    "# Load PragmatiCQA dataset\n",
    "def read_data(filename, dataset_dir=\"../PragmatiCQA/data\"):\n",
    "    corpus = []\n",
    "    with open(os.path.join(dataset_dir, filename), 'r') as f:\n",
    "        for line in f:\n",
    "            corpus.append(json.loads(line))\n",
    "    return corpus\n",
    "\n",
    "class TraditionalQA:\n",
    "    def __init__(self):\n",
    "        self.qa_pipeline = qa_pipeline\n",
    "\n",
    "    def answer_from_context(self, question, context):\n",
    "        result = self.qa_pipeline(question=question, context=context)\n",
    "        return result['answer']\n",
    "    \n",
    "# Load the datasets\n",
    "test_data = read_data(\"test.jsonl\")\n",
    "val_data = read_data(\"val.jsonl\")\n",
    "\n",
    "# Initialize the model\n",
    "traditional_qa = TraditionalQA()\n",
    "\n",
    "# Function to evaluate a configuration\n",
    "def evaluate_configuration(dataset, config):\n",
    "    examples = []\n",
    "    predictions = []\n",
    "\n",
    "    # Cache file for retrieved results\n",
    "    cache_file = \"retriever_cache.json\"\n",
    "    # Load cache if exists\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            retriever_cache = json.load(f)\n",
    "    else:\n",
    "        retriever_cache = {}\n",
    "\n",
    "    def get_cache_key(topic, question):\n",
    "        key_str = f\"{topic}|{question}\"\n",
    "        return hashlib.md5(key_str.encode()).hexdigest()\n",
    "\n",
    "    for conversation in dataset:\n",
    "        topic = conversation['topic']\n",
    "        if topic not in folders:\n",
    "            continue\n",
    "\n",
    "        first_qa = conversation['qas'][0]\n",
    "        question = first_qa['q']\n",
    "        gold_answer = first_qa['a']\n",
    "\n",
    "        if config == \"Literal\":\n",
    "            lit_spans = [l['text'] for l in first_qa['a_meta']['literal_obj']]\n",
    "            context = ' '.join(lit_spans)\n",
    "            pred_answer = traditional_qa.answer_from_context(question, context)\n",
    "        elif config == \"Pragmatic\":\n",
    "            prag_spans = [l['text'] for l in first_qa['a_meta']['pragmatic_obj']]\n",
    "            context = ' '.join(prag_spans)\n",
    "            pred_answer = traditional_qa.answer_from_context(question, context)\n",
    "        else:  # Retrieved]\n",
    "            cache_key = get_cache_key(topic, question)\n",
    "            if cache_key in retriever_cache:\n",
    "                pred_answer = retriever_cache[cache_key]\n",
    "            else:\n",
    "                search = make_search(topic)\n",
    "                passages = search(question).passages\n",
    "                context = \" \".join(passages)\n",
    "                pred_answer = traditional_qa.answer_from_context(question, context)\n",
    "                retriever_cache[cache_key] = pred_answer\n",
    "                \n",
    "                with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(retriever_cache, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        example = dspy.Example(question=question, response=gold_answer)\n",
    "        pred = dspy.Example(question=question, response=pred_answer)\n",
    "\n",
    "        examples.append(example)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    return examples, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93f044a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the model on the PRAGMATICQA test set...\n",
      "Evaluating configuration: Literal\n",
      "Evaluating configuration: Pragmatic\n",
      "Evaluating configuration: Retrieved\n",
      "\n",
      "Sample predictions:\n",
      "\n",
      "Example 1 - Topic: The Legend of Zelda\n",
      "Question: What year did the Legend of Zelda come out?\n",
      "Gold Answer: The Legend of Zelda came out as early as 1986 for the Famicom in Japan, and was later released in the western world, including Europe and the US in 1987. Would you like to know about the story?\n",
      "Literal Prediction: 1986\n",
      "Pragmatic Prediction: 1986\n",
      "Retrieved Prediction: 1986\n",
      "\n",
      "Example 2 - Topic: The Legend of Zelda\n",
      "Question: What console is The Legend of Zelda designed for?\n",
      "Gold Answer: The Legend of Zelda was originally released in 1986 for the Famicom in Japan and in 1987 for the NES in Europe and the US. Since it's release, The Legend of Zelda has been re-released several times, including ports for the GameCube and Game Boy Advance. The demand for these ports comes from the commercial success of Zelda, selling millions of copies nearly a year after it's initial release, the 4th best-selling NES game of all time.\n",
      "Literal Prediction: Famicom\n",
      "Pragmatic Prediction: Nintendo Entertainment System\n",
      "Retrieved Prediction: Game Boy Color\n",
      "\n",
      "Example 3 - Topic: The Legend of Zelda\n",
      "Question: when did the legend of zelda last until?\n",
      "Gold Answer: The Legend of Zelda is the first installment in the Zelda franchise, and its success allowed the development of sequels with nearly every title in the series influenced by this one. The latest installment of the series was with Nintendo Switch Online, released last on April 23, 2019.\n",
      "Literal Prediction: first installment in the Zelda franchise\n",
      "Pragmatic Prediction: April 23, 2019\n",
      "Retrieved Prediction: 1986\n",
      "\n",
      "Example 4 - Topic: The Legend of Zelda\n",
      "Question: When was the Legend of Zelda released?\n",
      "Gold Answer: The Legend of Zelda was released on August 22nd, 1987, for the Nintendo Entertainment System but was released as early as 1986 for the Famicon in Japan.\n",
      "Literal Prediction: August 22, 1987\n",
      "Pragmatic Prediction: 1987\n",
      "Retrieved Prediction: 1986\n",
      "\n",
      "Example 5 - Topic: The Legend of Zelda\n",
      "Question: What kind of game is The Legend of Zelda?\n",
      "Gold Answer: The Legend of Zelda is one that includes roleplaying, action, adventure, and puzzle/logic. It is the first installment of the Zelda series and centers its plot around a boy named Link.\n",
      "Literal Prediction: Zelda\n",
      "Pragmatic Prediction: roleplaying\n",
      "Retrieved Prediction: commercially successful\n",
      "\n",
      "Evaluating predictions using SemanticF1...\n",
      "\n",
      "Test set results:\n",
      "Configuration | F1 Score | Count\n",
      "-----------------------------------\n",
      "Literal       | 0.4274   | 120\n",
      "Pragmatic     | 0.3544   | 120\n",
      "Retrieved     | 0.1223   | 120\n"
     ]
    }
   ],
   "source": [
    "print(\"Running the model on the PRAGMATICQA test set...\")\n",
    "test_predictions = {}\n",
    "for config in [\"Literal\", \"Pragmatic\", \"Retrieved\"]:\n",
    "    print(f\"Evaluating configuration: {config}\")\n",
    "    examples, predictions = evaluate_configuration(test_data, config)\n",
    "    test_predictions[config] = {\n",
    "        'examples': examples,\n",
    "        'predictions': predictions,\n",
    "    }\n",
    "\n",
    "# Show some sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "for i in range(5):\n",
    "    conversation = test_data[i]\n",
    "    topic = conversation['topic']\n",
    "    first_qa = conversation['qas'][0]\n",
    "    question = first_qa['q']\n",
    "    gold_answer = first_qa['a']\n",
    "\n",
    "    print(f\"\\nExample {i+1} - Topic: {topic}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Gold Answer: {gold_answer}\")\n",
    "\n",
    "    for config in [\"Literal\", \"Pragmatic\", \"Retrieved\"]:\n",
    "        pred_answer = test_predictions[config]['predictions'][i].response\n",
    "        print(f\"{config} Prediction: {pred_answer}\")\n",
    "\n",
    "\n",
    "# Evaluate the predictions using SemanticF1\n",
    "print(\"\\nEvaluating predictions using SemanticF1...\")\n",
    "metric = SemanticF1(decompositional=True)\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for config in [\"Literal\", \"Pragmatic\", \"Retrieved\"]:\n",
    "    examples = test_predictions[config]['examples']\n",
    "    predictions = test_predictions[config]['predictions']\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for example, prediction in zip(examples, predictions):\n",
    "        score = metric(example, prediction)\n",
    "        f1_scores.append(score)\n",
    "\n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    test_results[config] = {\n",
    "        'f1': avg_f1, \n",
    "        'count': len(examples)\n",
    "    }\n",
    "\n",
    "# Display test results table\n",
    "print(\"\\nTest set results:\")\n",
    "print(\"Configuration | F1 Score | Count\")\n",
    "print(\"-\" * 35)\n",
    "for config, results in test_results.items():\n",
    "    print(f\"{config:<13} | {results['f1']:.4f}   | {results['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43925c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the model on the PRAGMATICQA val set...\n",
      "Evaluating configuration: Literal\n",
      "Evaluating configuration: Pragmatic\n",
      "Evaluating configuration: Retrieved\n",
      "\n",
      "Sample predictions:\n",
      "\n",
      "Example 1 - Topic: A Nightmare on Elm Street (2010 film)\n",
      "Question: who is freddy krueger?\n",
      "Gold Answer: Freddy Kruger is the nightmare in nighmare on Elm street. Please note, and to be very clear, the system that loads up wiki is not allowing access to Adam Prag, to the page... so I'll have to go from memory.  Normally you can paste things and back up what you are saying, but today that's not happening. alas.\n",
      "Literal Prediction: Gotham City socialites\n",
      "Pragmatic Prediction: his parents were killed by a small-time criminal named Joe Chill\n",
      "Retrieved Prediction: The most consistent\n",
      "\n",
      "Example 2 - Topic: A Nightmare on Elm Street (2010 film)\n",
      "Question: who was the star on this movie?\n",
      "Gold Answer: Robert Englund IS Freddy Kruger, the bad guy for these films. Note to you and to Adam, the Pragmatic one, the link here is broken and I can't paste relevant things, as has always been Nightmare's case, I'm perfectly good with answering your questions and will quickly do it, but have to open a tab in another window separate from the hit, I WILL go quickly and answer at rapid speed though, don't worry.\n",
      "Literal Prediction: Bruce Wayne\n",
      "Pragmatic Prediction: Bruce Wayne\n",
      "Retrieved Prediction: Barbara Gordon\n",
      "\n",
      "Example 3 - Topic: A Nightmare on Elm Street (2010 film)\n",
      "Question: What is the movie about?\n",
      "Gold Answer: Ok, here goes, I'm getting \"Cannot get\"..so, Nightmare on Elm street centers around the fact that in your dreams, Freddie Kruger, a dark figure can chase you and if you are killed while sleeping you die.\n",
      "Literal Prediction: I don't know\n",
      "Pragmatic Prediction: Bruce\n",
      "Retrieved Prediction: 23\n",
      "\n",
      "Example 4 - Topic: A Nightmare on Elm Street (2010 film)\n",
      "Question: Who directed the new film?\n",
      "Gold Answer: It was Directed by: Samuel Bayer. Note that the link here is broken. So I'm having to get some of this from memory. I copied what I have (this is ALL I have).\n",
      "Literal Prediction: No\n",
      "Pragmatic Prediction: intellect\n",
      "Retrieved Prediction: does not possess any superpowers\n",
      "\n",
      "Example 5 - Topic: Batman\n",
      "Question: Is the Batman comic similar to the movies?\n",
      "Gold Answer: I would say the movie and comics has same story line, as Batmans parents were the most wealthy folks in Gotham city, and they were killd while returning from a function by a small time criminal called Joe Chill\n",
      "Literal Prediction: Catwoman\n",
      "Pragmatic Prediction: Catwoman\n",
      "Retrieved Prediction: Lex Luthor\n",
      "\n",
      "Evaluating predictions using SemanticF1...\n",
      "Evaluating configuration: Literal\n",
      "Evaluating configuration: Pragmatic\n",
      "Evaluating configuration: Retrieved\n",
      "\n",
      "Validation set results:\n",
      "Configuration | Precision | Recall | F1 Score | Count\n",
      "------------------------------------------------------------\n",
      "Literal       | 0.8617   | 0.2738  | 0.4003   | 135\n",
      "Pragmatic     | 0.8037   | 0.2585  | 0.3714   | 135\n",
      "Retrieved     | 0.0914   | 0.0324  | 0.0394   | 135\n",
      "\n",
      "Analysis:\n",
      "Best configuration on test set: Literal with F1 score 0.4274\n",
      "Best configuration on validation set: Literal with F1 score 0.4003\n"
     ]
    }
   ],
   "source": [
    "print(\"Running the model on the PRAGMATICQA val set...\")\n",
    "val_predictions = {}\n",
    "for config in [\"Literal\", \"Pragmatic\", \"Retrieved\"]:\n",
    "    print(f\"Evaluating configuration: {config}\")\n",
    "    examples, predictions = evaluate_configuration(val_data, config)\n",
    "    val_predictions[config] = {\n",
    "        'examples': examples,\n",
    "        'predictions': predictions,\n",
    "    }\n",
    "\n",
    "# Show some sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "for i in range(5):\n",
    "    conversation = val_data[i]\n",
    "    topic = conversation['topic']\n",
    "    first_qa = conversation['qas'][0]\n",
    "    question = first_qa['q']\n",
    "    gold_answer = first_qa['a']\n",
    "\n",
    "    print(f\"\\nExample {i+1} - Topic: {topic}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Gold Answer: {gold_answer}\")\n",
    "\n",
    "    for config in [\"Literal\", \"Pragmatic\", \"Retrieved\"]:\n",
    "        pred_answer = val_predictions[config]['predictions'][i].response\n",
    "        print(f\"{config} Prediction: {pred_answer}\")\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "# Wrap the forward method to extract scores manually\n",
    "def patched_forward(self, example, pred, trace=None):\n",
    "    scores = self.module(\n",
    "        question=example.question,\n",
    "        ground_truth=example.response,\n",
    "        system_response=pred.response\n",
    "    )\n",
    "    f1 = f1_score(scores.precision, scores.recall)\n",
    "    return {\n",
    "        \"precision\": scores.precision,\n",
    "        \"recall\": scores.recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Evaluate the predictions using SemanticF1\n",
    "print(\"\\nEvaluating predictions using SemanticF1...\")\n",
    "metric = SemanticF1(decompositional=True)\n",
    "metric.forward = types.MethodType(patched_forward, metric)\n",
    "\n",
    "val_results = {}\n",
    "\n",
    "score_cache_file = \"val_score_cache.json\"\n",
    "if os.path.exists(score_cache_file):\n",
    "    with open(score_cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        score_cache = json.load(f)\n",
    "else:\n",
    "    score_cache = {}\n",
    "\n",
    "for config in [\"Literal\", \"Pragmatic\", \"Retrieved\"]:\n",
    "    print(f\"Evaluating configuration: {config}\")\n",
    "    examples = val_predictions[config]['examples']\n",
    "    predictions = val_predictions[config]['predictions']\n",
    "\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for example, prediction in zip(examples, predictions):\n",
    "        # Create a unique key for each example-prediction pair\n",
    "        cache_key = f\"{config}|{example.question}|{prediction.response}\"\n",
    "\n",
    "        if cache_key not in score_cache:\n",
    "            result = metric(example, prediction)\n",
    "            score = [result[\"precision\"], result[\"recall\"], result[\"f1\"]]\n",
    "            score_cache[cache_key] = score\n",
    "            # Save the scores to a JSON file\n",
    "            with open(score_cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(score_cache, f, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            score = score_cache[cache_key]\n",
    "\n",
    "        # Extract precision, recall, F1 if available in decomposed format\n",
    "        precision = score[0]\n",
    "        recall = score[1]\n",
    "        f1 = score[2]\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "    avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    val_results[config] = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1': avg_f1,\n",
    "        'count': len(examples)\n",
    "    }\n",
    "\n",
    "# Display validation results table\n",
    "print(\"\\nValidation set results:\")\n",
    "print(\"Configuration | Precision | Recall | F1 Score | Count\")\n",
    "print(\"-\" * 60)\n",
    "for config, results in val_results.items():\n",
    "    print(f\"{config:<13} | {results['precision']:.4f}   | {results['recall']:.4f}  | {results['f1']:.4f}   | {results['count']}\")\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "best_test_config = max(test_results, key=lambda k: test_results[k]['f1'])\n",
    "best_val_config = max(val_results, key=lambda k: val_results[k]['f1'])\n",
    "\n",
    "print(f\"Best configuration on test set: {best_test_config} with F1 score {test_results[best_test_config]['f1']:.4f}\")\n",
    "print(f\"Best configuration on validation set: {best_val_config} with F1 score {val_results[best_val_config]['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82457cf",
   "metadata": {},
   "source": [
    "- The model tends to give literal answers when a more pragmatic one is needed. For example, in the first example in the test set, the model provided a literal answer of \"1986\" instead of the more informative and pragmatic answer that included the release dates in different regions and an invitation for further discussion about the story. \n",
    "- In other examples, the model often fails to provide the full context or additional information that would make the answer more engaging and helpful and sometimes even provides incorrect or irrelevant answers, such as in the case of the Batman comic question where it answered with \"Catwoman\" instead of addressing the question about the similarity between the comic and the movies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2243e",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230431b8",
   "metadata": {},
   "source": [
    "### 4.4. Part 2: The LLM Multi-Step Prompting Approach\n",
    "Now, you will build a more sophisticated model using an LLM with multi-step prompting.\n",
    "\n",
    "We will now evaluate all the questions in the conversations, not only the first question of each conversation as in 4.3.\n",
    "\n",
    "In each turn, the model will have access as input to the following:\n",
    "* The previous turns as pairs (question, answer)\n",
    "* The current question.\n",
    "* The context retrieved by the retriever model given the current question.\n",
    "\n",
    "Implement the DSPy Module: Create a DSPy module that uses the strategy you devise to generate a cooperative answer.\n",
    "\n",
    "For reference, you can start from the DSPy tutorials demonstrating variations around RAG:\n",
    "* https://dspy.ai/tutorials/rag/ (using a retriever based on FAISS and passage embeddings)\n",
    "* https://dspy.ai/tutorials/multihop_search/ (using a retriever based on BM25s)\n",
    "* https://dspy.ai/tutorials/agents/ (using a ReACT module with a ColbertV2 retriever)\n",
    "\n",
    "\n",
    "#### 4.4.1 First Questions\n",
    "\n",
    "Perform the same evaluation as in 4.3 on the first questions in each conversation and compare the results of your model with the one in 4.3 based on the traditional text-to-text transformer.\n",
    "\n",
    "\n",
    "#### 4.4.2 Conversational Context\n",
    "\n",
    "Now consider all questions in the conversations and take into account the conversational context.\n",
    "\n",
    "Compile and Evaluate: Compile your DSPy program (you can use a small training set from the PRAGMATICQA data for this) and evaluate it on the validation set using the same metrics as in 4.3.  Explain which metric you use to drive the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40c8c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MultistepQA module\n",
    "class MultistepQA(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.respond = dspy.ChainOfThought(\"context, conversation_history, question -> answer\")\n",
    "\n",
    "    def forward(self, question, context=\"\", conversation_history=\"\"):\n",
    "        result = self.respond(\n",
    "            context=context,\n",
    "            conversation_history=conversation_history,\n",
    "            question=question\n",
    "        )\n",
    "        return result.answer\n",
    "    \n",
    "# Function to format conversation history as pairs of question and answer\n",
    "def format_conversation_history(qas_so_far):\n",
    "    history = []\n",
    "    for qa in qas_so_far:\n",
    "        history.append(f\"Question: {qa['q']} \\nAnswer: {qa['a']}\")\n",
    "    return \"\\n\\n\".join(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c53babf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4.1 - Evalute the model in first questions (samw as part 1)\n",
    "def evaluate_llm_first_question(dataset):\n",
    "    examples = []\n",
    "    predictions = []\n",
    "\n",
    "    cache_file = \"llm_first_question_cache.json\"\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            cache = json.load(f)\n",
    "    else:\n",
    "        cache = {}\n",
    "\n",
    "    # Initialize the multistep prompting module\n",
    "    multistep_qa = MultistepQA()\n",
    "\n",
    "    for conversation in dataset:\n",
    "        topic = conversation['topic']\n",
    "        if topic not in folders:\n",
    "            continue\n",
    "        \n",
    "        first_qa = conversation['qas'][0]\n",
    "        question = first_qa['q']\n",
    "        gold_answer = first_qa['a']\n",
    "\n",
    "        cache_key = f\"{topic}|{question}\"\n",
    "\n",
    "        if cache_key not in cache:\n",
    "            search = make_search(topic)\n",
    "            passages = search(question).passages\n",
    "            context = \" \".join(passages)\n",
    "\n",
    "            pred_answer = multistep_qa(question=question, context=context, conversation_history=\"\")\n",
    "            cache[cache_key] = pred_answer\n",
    "\n",
    "            with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(cache, f, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            pred_answer = cache[cache_key]\n",
    "\n",
    "        example = dspy.Example(question=question, response=gold_answer)\n",
    "        prediction = dspy.Example(question=question, response=pred_answer)\n",
    "\n",
    "        examples.append(example)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return examples, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4.1 - Evaluating the model on the first question...\n",
      "Evaluating example: Is the Batman comic similar to the movies?\n",
      "Evaluating example: what is batman's real name?\n",
      "Evaluating example: How old was batman when he first became batman?\n",
      "Evaluating example: Does Batman Have super powers, like invisibility, or the ability to organically shoot a web from his hand?\n",
      "Evaluating example: Who are Batman's biggest enemies?\n",
      "Evaluating example: What is Batmans real name?\n",
      "Evaluating example: Ok, Is batman a superhero?\n",
      "Evaluating example: who is the hero in batman\n",
      "Evaluating example: When did Batman first appear?\n",
      "Evaluating example: Did Batman start with a book or a movie?\n",
      "Evaluating example: how old is batman?\n",
      "Evaluating example: how old is batman?\n",
      "Evaluating example: what is batman's real name? \n",
      "Evaluating example: Is batman s a superhero? \n",
      "Evaluating example: what year was it release? \n",
      "Evaluating example: Hi. When was the first Batman comic released?\n",
      "Evaluating example: When was the original batman released?\n",
      "Evaluating example: Batman\n",
      "Evaluating example: Who is Batman?\n",
      "Evaluating example: Who is batman?\n",
      "Evaluating example: What was the first piece of media to feature Batman\n",
      "Evaluating example: When the first Batman movie released?\n",
      "Evaluating example: what year was batman launched?\n",
      "Evaluating example: Hi. What is Batman's name?\n",
      "Evaluating example: When did the Batman comics first appear?\n",
      "Evaluating example: Does Batman have real wings? \n",
      "Evaluating example: what is the batmobile?\n",
      "Evaluating example: What is the latest in the Batman Series of movies?\n",
      "Evaluating example: Who was Batman's first villian?\n",
      "Evaluating example: I filled out the test & clicked submit.  \n",
      "Evaluating example: when was batman made?\n",
      "Evaluating example: what year was batman release? \n",
      "Evaluating example: When did Batman first appear in a comic book?\n",
      "Evaluating example: who is the star in batman?\n",
      "Evaluating example: What is Batman?\n",
      "Evaluating example: when was batman made\n",
      "Evaluating example: Hi! Is Batman a real human? \n",
      "Evaluating example: who played batman the most on tv?\n",
      "Evaluating example: what year was the show premiere?\n",
      "Evaluating example: What is the plot of the show?\n"
     ]
    }
   ],
   "source": [
    "print(\"4.4.1 - Evaluating the model on the first question...\")\n",
    "examples_first_question, predictions_first_question = evaluate_llm_first_question(val_data)\n",
    "\n",
    "llm_score_cache_file = \"llm_score_cache.json\"\n",
    "if os.path.exists(llm_score_cache_file):\n",
    "    with open(llm_score_cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        llm_score_cache = json.load(f)\n",
    "else:\n",
    "    llm_score_cache = {}\n",
    "\n",
    "# Evaluate the predictions using SemanticF1\n",
    "llm_first_scores = {\"precision\": [], \"recall\": [], \"f1\": []}\n",
    "for example, prediction in zip(examples_first_question, predictions_first_question):\n",
    "    print(f\"Evaluating example: {example.question}\")\n",
    "    cache_key = f\"LLM_first|{example.question}|{prediction.response}\"\n",
    "\n",
    "    if cache_key not in llm_score_cache:\n",
    "        result = metric(example, prediction)\n",
    "        score = [result[\"precision\"], result[\"recall\"], result[\"f1\"]]\n",
    "        llm_score_cache[cache_key] = score\n",
    "\n",
    "        with open(llm_score_cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(llm_score_cache, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        score = llm_score_cache[cache_key]\n",
    "\n",
    "    llm_first_scores[\"precision\"].append(score[0])\n",
    "    llm_first_scores[\"recall\"].append(score[1])\n",
    "    llm_first_scores[\"f1\"].append(score[2])\n",
    "\n",
    "llm_first_results = {\n",
    "    'precision': sum(llm_first_scores[\"precision\"]) / len(llm_first_scores[\"precision\"]),\n",
    "    'recall': sum(llm_first_scores[\"recall\"]) / len(llm_first_scores[\"recall\"]),\n",
    "    'f1': sum(llm_first_scores[\"f1\"]) / len(llm_first_scores[\"f1\"]),\n",
    "    'count': len(examples_first_question)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d77859f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4.2 - Evaluate the model on all questions with conversation history\n",
    "def evaluate_llm_all_questions(dataset):\n",
    "    examples = []\n",
    "    predictions = []\n",
    "\n",
    "    cache_file = \"llm_all_questions_cache.json\"\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            cache = json.load(f)\n",
    "    else:\n",
    "        cache = {}\n",
    "\n",
    "    search_cache = {}\n",
    "    \n",
    "    # Initialize the multistep prompting module\n",
    "    multistep_qa = MultistepQA()\n",
    "\n",
    "    for conversation in dataset:\n",
    "        topic = conversation['topic']\n",
    "        if topic not in folders:\n",
    "            continue\n",
    "\n",
    "        if topic not in search_cache:\n",
    "            search = make_search(topic)\n",
    "            search_cache[topic] = search    \n",
    "        else:\n",
    "            search = search_cache[topic]\n",
    "        \n",
    "        conversation_so_far = []\n",
    "\n",
    "        for qa_index, qa in enumerate(conversation['qas']):\n",
    "            question = qa['q']\n",
    "            gold_answer = qa['a']\n",
    "\n",
    "            conversation_history = format_conversation_history(conversation_so_far)\n",
    "\n",
    "            cache_key = f\"{topic}|{qa_index + 1}|{question}\"\n",
    "\n",
    "            if cache_key not in cache:\n",
    "                passages = search(question).passages\n",
    "                context = \" \".join(passages)\n",
    "\n",
    "                pred_answer = multistep_qa(question=question, context=context, conversation_history=conversation_history)\n",
    "                cache[cache_key] = pred_answer\n",
    "\n",
    "                with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(cache, f, ensure_ascii=False, indent=2)\n",
    "            else:\n",
    "                pred_answer = cache[cache_key]\n",
    "\n",
    "            example = dspy.Example(question=question, response=gold_answer)\n",
    "            prediction = dspy.Example(question=question, response=pred_answer)\n",
    "\n",
    "            examples.append(example)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "            conversation_so_far.append({'q': question, 'a': gold_answer})\n",
    "\n",
    "    return examples, predictions\n",
    "\n",
    "examples_all_questions, predictions_all_questions = evaluate_llm_all_questions(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced94291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
