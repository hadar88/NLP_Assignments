{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres Baseline\n",
    "\n",
    "This notebook illustrates how to use the DeBERTa-v3-base-mnli-fever-anli model to perform specialized inference on the ImpPres dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cfe31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "def evaluate(premise, hypothesis):\n",
    "    input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(input[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2954d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'entailment': 0.1, 'neutral': 99.8, 'contradiction': 0.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"The weather is nice today.\", \"It is sunny outside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "923ea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(pred_dict):\n",
    "    if pred_dict[\"entailment\"] > pred_dict[\"contradiction\"]  and pred_dict[\"entailment\"] > pred_dict[\"neutral\"]:\n",
    "        return \"entailment\"\n",
    "    elif pred_dict[\"contradiction\"] > pred_dict[\"entailment\"]:\n",
    "        return \"contradiction\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ImpPres Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0438789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for section: presupposition_all_n_presupposition\n",
      "Loading dataset for section: presupposition_both_presupposition\n",
      "Loading dataset for section: presupposition_change_of_state\n",
      "Loading dataset for section: presupposition_cleft_existence\n",
      "Loading dataset for section: presupposition_cleft_uniqueness\n",
      "Loading dataset for section: presupposition_only_presupposition\n",
      "Loading dataset for section: presupposition_possessed_definites_existence\n",
      "Loading dataset for section: presupposition_possessed_definites_uniqueness\n",
      "Loading dataset for section: presupposition_question_presupposition\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sections = ['presupposition_all_n_presupposition', \n",
    "            'presupposition_both_presupposition', \n",
    "            'presupposition_change_of_state', \n",
    "            'presupposition_cleft_existence', \n",
    "            'presupposition_cleft_uniqueness', \n",
    "            'presupposition_only_presupposition', \n",
    "            'presupposition_possessed_definites_existence', \n",
    "            'presupposition_possessed_definites_uniqueness', \n",
    "            'presupposition_question_presupposition']\n",
    "\n",
    "dataset = {}\n",
    "for section in sections:\n",
    "    print(f\"Loading dataset for section: {section}\")\n",
    "    dataset[section] = load_dataset(\"facebook/imppres\", section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presupposition_all_n_presupposition': DatasetDict({\n",
       "     all_n_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_both_presupposition': DatasetDict({\n",
       "     both_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_change_of_state': DatasetDict({\n",
       "     change_of_state: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_cleft_existence': DatasetDict({\n",
       "     cleft_existence: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_cleft_uniqueness': DatasetDict({\n",
       "     cleft_uniqueness: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_only_presupposition': DatasetDict({\n",
       "     only_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_possessed_definites_existence': DatasetDict({\n",
       "     possessed_definites_existence: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_possessed_definites_uniqueness': DatasetDict({\n",
       "     possessed_definites_uniqueness: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_question_presupposition': DatasetDict({\n",
       "     question_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8262068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the ImpPres dataset\n",
    "from tqdm import tqdm\n",
    "def evaluate_on_dataset(dataset):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    for example in tqdm(dataset):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = evaluate(premise, hypothesis)\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'prediction': prediction,\n",
    "            'pred_label': get_prediction(prediction),\n",
    "            'gold_label': label_names[example['gold_label']],\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline model on each section of the ImpPres dataset.\n",
    "\n",
    "https://www.kaggle.com/code/faijanahamadkhan/llm-evaluation-framework-hugging-face provides good documentation on how to use the Huggingface evaluate library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c57fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_all_n_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_both_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_change_of_state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_cleft_existence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_cleft_uniqueness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_only_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_possessed_definites_existence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_possessed_definites_uniqueness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_question_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Section                        Accuracy Precision Recall F1  \n",
      "          all_n_presupposition 0.60     0.48      0.58   0.49\n",
      "           both_presupposition 0.30     0.17      0.33   0.22\n",
      "               change_of_state 0.00     0.00      0.00   0.00\n",
      "               cleft_existence 0.60     0.50      0.58   0.51\n",
      "              cleft_uniqueness 0.10     0.17      0.08   0.11\n",
      "           only_presupposition 0.70     0.50      0.67   0.56\n",
      " possessed_definites_existence 0.70     0.50      0.67   0.56\n",
      "possessed_definites_uniqueness 0.30     0.14      0.33   0.20\n",
      "       question_presupposition 0.70     0.50      0.67   0.56\n",
      "                       Overall 0.44     0.33      0.44   0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "results_table = []\n",
    "\n",
    "for section in sections:\n",
    "    print(f\"Working on section: {section}\")\n",
    "    sec = section[15:]\n",
    "\n",
    "    data = dataset[section]\n",
    "    data = data[sec]\n",
    "    \n",
    "    section_results = evaluate_on_dataset(data)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    predictions = [result['pred_label'] for result in section_results]\n",
    "    references = [result['gold_label'] for result in section_results]\n",
    "\n",
    "    label_to_int = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "    predictions_int = [label_to_int[pred] for pred in predictions]\n",
    "    references_int = [label_to_int[ref] for ref in references]\n",
    "\n",
    "    accuracy_score = accuracy.compute(predictions=predictions_int, references=references_int)['accuracy']\n",
    "    f1_score = f1.compute(predictions=predictions_int, references=references_int, average='macro')['f1']\n",
    "    precision_score = precision.compute(predictions=predictions_int, references=references_int, average='macro', zero_division=0)['precision']\n",
    "    recall_score = recall.compute(predictions=predictions_int, references=references_int, average='macro', zero_division=0)['recall']\n",
    "    \n",
    "    accuracies.append(accuracy_score)\n",
    "    precisions.append(precision_score)\n",
    "    recalls.append(recall_score)\n",
    "    f1s.append(f1_score)\n",
    "\n",
    "    # Append results to the table\n",
    "    results_table.append({\n",
    "        'Section': sec,\n",
    "        'Accuracy': f\"{accuracy_score:.2f}\",\n",
    "        'Precision': f\"{precision_score:.2f}\",\n",
    "        'Recall': f\"{recall_score:.2f}\",\n",
    "        'F1': f\"{f1_score:.2f}\",\n",
    "    })\n",
    "\n",
    "# Calculate overall metrics\n",
    "accuracy_all = sum(accuracies) / len(accuracies)\n",
    "precision_all = sum(precisions) / len(precisions)\n",
    "recall_all = sum(recalls) / len(recalls)\n",
    "f1_all = sum(f1s) / len(f1s)\n",
    "\n",
    "results_table.append({\n",
    "    'Section': 'Overall',\n",
    "    'Accuracy': f\"{accuracy_all:.2f}\",\n",
    "    'Precision': f\"{precision_all:.2f}\",\n",
    "    'Recall': f\"{recall_all:.2f}\",\n",
    "    'F1': f\"{f1_all:.2f}\",\n",
    "})\n",
    "\n",
    "# Display results as a table\n",
    "results_df = pd.DataFrame(results_table)\n",
    "print(\"\\n\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
