{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline\n",
    "\n",
    "This model illustrates how to use the DeBERTa-v3-base-mnli-fever-anli model to perform specialized inference on the ANLI dataset.\n",
    "This dataset has 184M parameters. It was trained in 2021 on the basis of a BERT-like embedding approach: \n",
    "* The premise and the hypothesis are encoded using the DeBERTa-v3-base contextual encoder\n",
    "* The encodings are then compared on a fine-tuned model to predict a distribution over the classification labels (entailment, contradiction, neutral)\n",
    "\n",
    "Reported accuracy on ANLI is 0.495 (see https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80a47aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment': 6.6, 'neutral': 17.3, 'contradiction': 76.1}\n"
     ]
    }
   ],
   "source": [
    "premise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\n",
    "hypothesis = \"The movie was good.\"\n",
    "\n",
    "input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0cfe31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(premise, hypothesis):\n",
    "    input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(input[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a2954d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0.1, 'neutral': 99.8, 'contradiction': 0.0}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"The weather is nice today.\", \"It is sunny outside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "923ea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(pred_dict):\n",
    "    if pred_dict[\"entailment\"] > pred_dict[\"contradiction\"]  and pred_dict[\"entailment\"] > pred_dict[\"neutral\"]:\n",
    "        return \"entailment\"\n",
    "    elif pred_dict[\"contradiction\"] > pred_dict[\"entailment\"]:\n",
    "        return \"contradiction\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af257dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"The weather is nice today.\", \"It is sunny outside.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "929632f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entailment'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is nice today.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "747c0cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is terrible today.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8262068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the ANLI dataset\n",
    "from tqdm import tqdm\n",
    "def evaluate_on_dataset(dataset):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    for example in tqdm(dataset):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = evaluate(premise, hypothesis)\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'prediction': prediction,\n",
    "            'pred_label': get_prediction(prediction),\n",
    "            'gold_label': label_names[example['label']],\n",
    "            'reason': example['reason']\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f858feae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [06:56<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_test_r3 = evaluate_on_dataset(dataset['test_r3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c8efb717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'premise': \"It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of the articles this week deal with the iPhone, its future version called the iPhone 8 or iPhone Edition, and new builds of iOS and macOS. There are also some posts that deal with the iPhone rival called the Galaxy S8 and some other interesting stories. The list of the most interesting articles is available below. Stay tuned for more rumors and don't forget to follow us on Twitter.\",\n",
       "  'hypothesis': 'The day of the passage is usually when Christians praise the lord together',\n",
       "  'prediction': {'entailment': 2.4, 'neutral': 97.4, 'contradiction': 0.2},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': \"Sunday is considered Lord's Day\"},\n",
       " {'premise': 'By The Associated Press WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. Copyright © 2018 The Associated Press. All rights reserved. This material may not be published, broadcast, written or redistributed.',\n",
       "  'hypothesis': 'No children were killed in the accident.',\n",
       "  'prediction': {'entailment': 0.1, 'neutral': 99.9, 'contradiction': 0.0},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The context confirms that everybody survived the accident, so there is no way that a child was killed.'},\n",
       " {'premise': 'Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snacking with a gold-wrapped Kit Kat chocolate bar. The single finger Kit Kat is wrapped in a thin layer of gold leaf. Only 500 of the bars go on sale from Dec. 29 with a price tag of around 2,016 yen ($16). The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced.',\n",
       "  'hypothesis': 'Japanese like kit kat. ',\n",
       "  'prediction': {'entailment': 84.0, 'neutral': 15.9, 'contradiction': 0.1},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'according to the text, The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced, which means if  they have been so many produced it is because they like it. '},\n",
       " {'premise': 'Governor Greg Abbott has called for a statewide show of support for law enforcement Friday, July 7. Locally, a 15-minute program is planned at 9 a.m. at Memorial Lane Park, 550 N. Travis St. The governor is asking law enforcement officers to turn on red and blue flashing lights for one-minute at 10 a.m. Multiple law enforcement officers were shot and killed in Dallas one year ago.',\n",
       "  'hypothesis': 'Law enforcement officers and the people at the Travis St. memorial do not show their support at the same time.',\n",
       "  'prediction': {'entailment': 11.9, 'neutral': 75.8, 'contradiction': 12.3},\n",
       "  'pred_label': 'contradiction',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The Travis St.memorial program begins at 9 a.m. Law enforcement officers were asked to turn on red and blue flashing lights for one-minute at 10 a.m.'},\n",
       " {'premise': 'Sept 4 (Reuters) - J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France. Based in Paris, Grassano started in his new role on Sept. 1, J.P. Morgan Asset Management said in a statement. Grassano, who has been with the company since 2002, was previously the head of sales for Italy, covering wholesale and retail distribution. He has earlier worked at BNP Paribas Asset Management.',\n",
       "  'hypothesis': 'Pietro Grassano was once the country head for France.',\n",
       "  'prediction': {'entailment': 2.9, 'neutral': 55.1, 'contradiction': 42.0},\n",
       "  'pred_label': 'contradiction',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': '\"J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France.\" I think it was difficult because I worded it past tense, \"He was ONCE the country head\", but I believe that statement is true because it is past Sept 1 when he was appointed.'}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_r3[:5]  # Display the first 5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline model on each section of the ANLI dataset.\n",
    "\n",
    "https://www.kaggle.com/code/faijanahamadkhan/llm-evaluation-framework-hugging-face provides good documentation on how to use the Huggingface evaluate library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0905e831",
   "metadata": {},
   "source": [
    "## 1.1. Execute the NLI Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9de3aedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:56<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for section test_r1:\n",
      "\tAccuracy: 0.619\n",
      "\tF1: 0.605\n",
      "\tPrecision: 0.633\n",
      "\tRecall: 0.619\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:32<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for section test_r2:\n",
      "\tAccuracy: 0.504\n",
      "\tF1: 0.489\n",
      "\tPrecision: 0.508\n",
      "\tRecall: 0.504\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [06:47<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for section test_r3:\n",
      "\tAccuracy: 0.481\n",
      "\tF1: 0.463\n",
      "\tPrecision: 0.465\n",
      "\tRecall: 0.482\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def evaluate_all_test_sections(dataset):\n",
    "    results = {}\n",
    "    sections = ['test_r1', 'test_r2', 'test_r3']\n",
    "\n",
    "    for section in sections:\n",
    "        section_results = evaluate_on_dataset(dataset[section])\n",
    "        results[section] = section_results\n",
    "\n",
    "        predictions = [result['pred_label'] for result in section_results]\n",
    "        references = [result['gold_label'] for result in section_results]\n",
    "\n",
    "        #  Convert the lables to numerical values for evaluation\n",
    "        label_to_int = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "        predictions_int = [label_to_int[pred] for pred in predictions]\n",
    "        references_int = [label_to_int[ref] for ref in references]\n",
    "\n",
    "        accuracy_score = accuracy.compute(predictions=predictions_int, references=references_int)['accuracy']\n",
    "        f1_score = f1.compute(predictions=predictions_int, references=references_int, average='macro')['f1']\n",
    "        precision_score = precision.compute(predictions=predictions_int, references=references_int, average='macro')['precision']\n",
    "        recall_score = recall.compute(predictions=predictions_int, references=references_int, average='macro')['recall']\n",
    "\n",
    "        print(f\"Results for section {section}:\")\n",
    "        print(f\"\\tAccuracy: {accuracy_score:.3f}\")\n",
    "        print(f\"\\tF1: {f1_score:.3f}\")\n",
    "        print(f\"\\tPrecision: {precision_score:.3f}\")\n",
    "        print(f\"\\tRecall: {recall_score:.3f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return results\n",
    "\n",
    "test_results = evaluate_all_test_sections(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95bf2f3",
   "metadata": {},
   "source": [
    "## 1.2. Investigate Errors of the NLI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "41bb3465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 1:\n",
      "\tPremise: Shadowboxer is a 2005 crime thriller film directed by Lee Daniels and starring Academy Award winners Cuba Gooding Jr., Helen Mirren, and Mo'Nique. It opened in limited release in six cities: New York, Los Angeles, Washington, D.C., Baltimore, Philadelphia, and Richmond, Virginia.\n",
      "\tHypothesis: Shadowboxer was written and directed by Lee Daniels and was starring Academy Award winners Cuba Gooding Jr., Helen Mirren, and Mo'Nique.\n",
      "\tPredicted: entailment\n",
      "\tGold Label: neutral\n",
      "\tReason: It is not know who wrote the Shadowboxer. The system can get confused if a small detail is added for a person while many correct details are written.\n",
      "--------------------------------------------------\n",
      "Error 2:\n",
      "\tPremise: Michael T. Scuse (born 1954) is an American public official. He was the acting United States Deputy Secretary of Agriculture, and, following the resignation of Tom Vilsack on January 13, 2017, was acting United States Secretary of Agriculture until Donald Trump took office as President. He also served as Under Secretary of Agriculture for Farm and Foreign Agricultural Services since 2012.\n",
      "\tHypothesis: The US Secretary of Agriculture is assigned Secret Service for protection purposes.\n",
      "\tPredicted: contradiction\n",
      "\tGold Label: neutral\n",
      "\tReason: The priority of the Secretary's safety is not stated in the facts. While this position allows the individual to retain a seat within a President's cabinet, it is unknown if Secret Service protection is granted for the individual.\n",
      "--------------------------------------------------\n",
      "Error 3:\n",
      "\tPremise: Edmond (or Edmund) Halley, FRS (pronounced ; 8 November [O.S. 29 October] 1656 – 25 January 1742 [O.S. 14 January 1741] ) was an English astronomer, geophysicist, mathematician, meteorologist, and physicist who is best known for computing the orbit of Halley's Comet. He was the second Astronomer Royal in Britain, succeeding John Flamsteed.\n",
      "\tHypothesis: Edmond Halley was born outside of the United Kingdom. \n",
      "\tPredicted: contradiction\n",
      "\tGold Label: neutral\n",
      "\tReason: We know Edmond Halley is English, but we are not sure if he was actually born in the United Kingdom or not. \n",
      "--------------------------------------------------\n",
      "Error 4:\n",
      "\tPremise: Camassia cusickii, common name Cussick's camas, is a species of plant in the Asparagaceae family (subfamily Agavoideae). It is native to parts of North America. It has linear leaves with parallel venation and flowers in parts of three. The flowers are usually ice blue or baby blue in color, although they can be various shades of blue, cream and white.\n",
      "\tHypothesis: Crussicks camas can be found all over the world\n",
      "\tPredicted: contradiction\n",
      "\tGold Label: neutral\n",
      "\tReason: The statement provides the plants origin but not where it can be found in other parts of the world\n",
      "--------------------------------------------------\n",
      "Error 5:\n",
      "\tPremise: Dyllan McGee is a documentary filmmaker and founder of McGee Media. In partnership with Peter Kunhardt, McGee produced \"Gloria: In Her Own Words” (HBO), “Finding Your Roots with Henry Louis Gates, Jr.” (PBS), \"MAKERS: Women Who Make America” and many more. McGee is the Founder and Executive Producer of AOL’s MAKERS.\n",
      "\tHypothesis: D McGee is a filmmaker who founded their own company. They made many films, that were broadcast on many US networks. \n",
      "\tPredicted: neutral\n",
      "\tGold Label: entailment\n",
      "\tReason: The AI cannot know where the films were broadcast. \n",
      "--------------------------------------------------\n",
      "Error 6:\n",
      "\tPremise: The 2009–10 Tour de Ski was the 4th edition of the Tour de Ski and took place 1–10 January 2010. The race started in Oberhof, Germany, and ended in Val di Fiemme, Italy. The defending champions are Switzerland's Dario Cologna for the men and Finland's Virpi Kuitunen. This year's event was won by Lukáš Bauer of the Czech Republic for the men and Poland's Justyna Kowalczyk for the women.\n",
      "\tHypothesis: Dario Cologna won the 3rd edition of the Tour de Ski.\n",
      "\tPredicted: neutral\n",
      "\tGold Label: entailment\n",
      "\tReason: It said he was a defending champion which means he won the year before, making that the 3rd iteration of the race.\n",
      "--------------------------------------------------\n",
      "Error 7:\n",
      "\tPremise: Aram is a 2002 French action film. It takes place in France between 1993 and 2001, wherein French-Armenian fighters supply arms to Nagorno-Karabakh and kill a visiting Turkish general. The film was released in 2002 in theatres in France, and made its American debut in 2004 at the Armenian Film Festival in San Francisco.\n",
      "\tHypothesis: In 2002, the Armenian Film Festival was held in San Francisco.\n",
      "\tPredicted: contradiction\n",
      "\tGold Label: neutral\n",
      "\tReason: We don't know where or if there was even a 2002 Armenian film festival.\n",
      "--------------------------------------------------\n",
      "Error 8:\n",
      "\tPremise: The diocese of Vannida (in Latin: Dioecesis Vannidensis) is a suppressed and titular See of the Roman Catholic Church. It was centered on the ancient Roman Town of Vannida, in what is today Algeria, is an ancient episcopal seat of the Roman province of Mauritania Cesariense.\n",
      "\tHypothesis: The diocese of Vannida is located in Europe\n",
      "\tPredicted: entailment\n",
      "\tGold Label: contradiction\n",
      "\tReason: No it is in Algeria\n",
      "--------------------------------------------------\n",
      "Error 9:\n",
      "\tPremise: Daniel Zolnikov (born January 29, 1987) is a Republican member of the Montana Legislature. He was elected to House District 47 which represents Billings, Montana After redistricting, he now represents House District 45. He has made a name for himself pursuing pro-privacy legislation.\n",
      "\tHypothesis: There is no information indicating whether Daniel Zolnikov is a good legislator or not.\n",
      "\tPredicted: contradiction\n",
      "\tGold Label: neutral\n",
      "\tReason: The text did not provide any information regarding the legislator's competence\n",
      "--------------------------------------------------\n",
      "Error 10:\n",
      "\tPremise: San Marco 1, also known as San Marco A, was the first Italian satellite, and the first non-Soviet/US spacecraft. Built in-house by the Italian Space Research Commission (Italian: \"Commissione per le Ricerche Spaziali\" , CRS) on behalf of the National Research Council, it was the first of five as part of the Italian-US San Marco programme.\n",
      "\tHypothesis: Tthe Italian-US San Marco programme was deemed a failure.\n",
      "\tPredicted: contradiction\n",
      "\tGold Label: neutral\n",
      "\tReason: While the Italian-US San Marco programme did release five satellites, it was not clear if they were deemed as successful or as failures. I think the model was fooled because it assumed that because five were released that that meant it was a success.\n",
      "--------------------------------------------------\n",
      "Error 11:\n",
      "\tPremise: De Baandert was a multi-use stadium in Sittard-Geleen, Netherlands. It was used mostly for football matches and hosted the home matches of Fortuna Sittard. The stadium was able to hold 22,000 people. It was closed in 1999 when Fortuna Sittard Stadion opened.\n",
      "\tHypothesis: 22,000 people go to football matches at De Baandert.\n",
      "\tPredicted: entailment\n",
      "\tGold Label: neutral\n",
      "\tReason: The stadium can hold 22,000 people, but there is no guarantee that the matches will sell-out.\n",
      "--------------------------------------------------\n",
      "Error 12:\n",
      "\tPremise: Saleby is a village in the civil parish of Beesby with Saleby , in the East Lindsey district of Lincolnshire, England. It is on the Alford road to Louth, about 2 mi north-east of Alford and 11 mi south-east of Louth. The hamlet of Thoresthorpe is about 1 mi south of the village.\n",
      "\tHypothesis: Thoresthorpe is 1 mi south of Alford\n",
      "\tPredicted: entailment\n",
      "\tGold Label: contradiction\n",
      "\tReason: It is 1 mi south of Saleby. I think the system got it wrong because the context is worded ambiguously.\n",
      "--------------------------------------------------\n",
      "Error 13:\n",
      "\tPremise: The Death and Life of John F. Donovan is an upcoming Canadian drama film, co-written, co-produced and directed by Xavier Dolan in his English-language debut. It stars Kit Harington, Natalie Portman, Jessica Chastain, Susan Sarandon, Kathy Bates, Jacob Tremblay, Ben Schnetzer, Thandie Newton, Amara Karan, Chris Zylka, Jared Keeso, Emily Hampshire and Michael Gambon.\n",
      "\tHypothesis: Xavier Dolan has two different jobs in the creating of The Death and Life of John F. Donovan.\n",
      "\tPredicted: entailment\n",
      "\tGold Label: contradiction\n",
      "\tReason: Xavier Dolan has three jobs (not two) in the creating of The Death and Life of John F. Donovan. He is the co-writer, co-producer, and director. It may have been difficult because it is a bit confusing.\n",
      "--------------------------------------------------\n",
      "Error 14:\n",
      "\tPremise: This is a list of episodes of the British television situation comedy \"Lead Balloon\". The first series of six episodes aired in 2006 and a second series, extended to eight episodes, aired in 2007. The third series began in November 2008. All episodes are written by Jack Dee and Pete Sinclair, and are directed and produced by Alex Hardcastle.\n",
      "\tHypothesis: The third series of \"Lead Balloon\" went into 2009.\n",
      "\tPredicted: contradiction\n",
      "\tGold Label: neutral\n",
      "\tReason: This is neither definitely correct nor definitely incorrect because it said it started in 2008, it could have ended the same year if it had low ratings, and it is not known if it went into 2009. I think it was difficult for the system because the series began in 2008, so maybe it thought it didn't go beyond that.\n",
      "--------------------------------------------------\n",
      "Error 15:\n",
      "\tPremise: Woman Out of Control is second solo album by Ray Parker Jr. It was released in 1983 on the Arista label. The record includes the single \"I Still Can't Get Over Loving You\" which reached number 12 on the \"Billboard\" Hot 100 in early 1984. The album was remastered and expanded by Funky Town Grooves in June 2012.\n",
      "\tHypothesis: The song \"Ghostbusters!\" is on Woman Out of Control.\n",
      "\tPredicted: contradiction\n",
      "\tGold Label: neutral\n",
      "\tReason: This Ray Parker, Jr. song might, or might not be on the album; the information simply is not included. I do not know why it was difficult for the system.\n",
      "--------------------------------------------------\n",
      "Error 16:\n",
      "\tPremise: WSJD (100.5 FM, \"True Oldies 100.5\") is a radio station serving the Evansville, Indiana area with an oldies format. It broadcasts on FM frequency 100.5 MHz and is under self ownership. Majority of the programming is featured from ABC Radio's \"The True Oldies Channel\" satellite feed. The station broadcasts Major League Baseball games as a member of the Los Angeles Dodgers Radio Network.\n",
      "\tHypothesis: Los Angeles Dodgers Radio Network want to separate from the WSJD \n",
      "\tPredicted: contradiction\n",
      "\tGold Label: neutral\n",
      "\tReason: you cannot know this informatin from the text\n",
      "--------------------------------------------------\n",
      "Error 17:\n",
      "\tPremise: The Centralia Massacre was an incident during the American Civil War in which twenty-four unarmed Union soldiers were captured and executed at Centralia, Missouri on September 27, 1864 by the pro-Confederate guerrilla leader William T. Anderson. Future outlaw Jesse James was among the guerrillas.\n",
      "\tHypothesis: William T Anderson was Jesse James superior at one point.\n",
      "\tPredicted: neutral\n",
      "\tGold Label: entailment\n",
      "\tReason: If Anderson was the guerrilla leader and Jesse James was one of the guerrillas then he was under command of Anderson. System could not predict because there wasn't enough clue words in the statement\n",
      "--------------------------------------------------\n",
      "Error 18:\n",
      "\tPremise: Meg Randall (born \"Genevieve Roberts\"; August 1, 1926 in Clinton, Oklahoma) was an American film actress who also attended the University of Oklahoma as an undergraduate, completing only her freshman year. She was active in motion pictures, radio and television between 1946 and 1961, changing her name from Gene Roberts to Meg Randall in mid-1948.\n",
      "\tHypothesis: Meg Randall did not study acting\n",
      "\tPredicted: contradiction\n",
      "\tGold Label: neutral\n",
      "\tReason: Its not clear what her major was\n",
      "--------------------------------------------------\n",
      "Error 19:\n",
      "\tPremise: Titus is the original soundtrack to the 1999 motion picture \"Titus\". Elliot Goldenthal wrote the score for the film, an adaptation of Shakespeare's first, and bloodiest, tragedy \"Titus Andronicus\"; written and directed by Julie Taymor, Goldenthal's long-time friend and partner. The only non-Goldenthal piece is an old Italian song called \"\"Vivere\"\" performed by Italian singer Carlo Buti.\n",
      "\tHypothesis: Vivere is the only piece on the Titus soundtrack performed by Carlo Buti\n",
      "\tPredicted: entailment\n",
      "\tGold Label: neutral\n",
      "\tReason: While it is stated that all the other pieces were composed by Goldenthal, it is not stated who performed them so it could be Carlo Buti. I think the system got it wrong because the statement seems like a logical assumption\n",
      "--------------------------------------------------\n",
      "Error 20:\n",
      "\tPremise: Me Before You is a 2016 romantic drama film directed by Thea Sharrock in her directorial debut and adapted by English author Jojo Moyes from her 2012 novel of the same name. The film stars Emilia Clarke, Sam Claflin, Steve Peacocke, Jenna Coleman, Charles Dance, Matthew Lewis, Janet McTeer, Vanessa Kirby and Joanna Lumley.\n",
      "\tHypothesis: Jojo Moyes wrote the screenplay for Me Before You in 2014.\n",
      "\tPredicted: contradiction\n",
      "\tGold Label: neutral\n",
      "\tReason: We know from the text that Jojo Moyes wrote the screenplay, and it had to be done sometime between the novel and the film, but it doesn't mention the date. The system thinks it is definitely incorrect because of the use of a date that is not in the text.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def sample_errors(test_results):\n",
    "    errors = []\n",
    "    \n",
    "    for section in ['test_r1', 'test_r2', 'test_r3']:\n",
    "        for result in test_results[section]:\n",
    "            if result['pred_label'] != result['gold_label']:\n",
    "                errors.append(result)\n",
    "\n",
    "            if len(errors) == 20:\n",
    "                return errors\n",
    "\n",
    "    return errors\n",
    "\n",
    "error_samples = sample_errors(test_results)\n",
    "for i, error in enumerate(error_samples):\n",
    "    print(f\"Error {i+1}:\")\n",
    "    print(f\"\\tPremise: {error['premise']}\")\n",
    "    print(f\"\\tHypothesis: {error['hypothesis']}\")\n",
    "    print(f\"\\tPredicted: {error['pred_label']}\")\n",
    "    print(f\"\\tGold Label: {error['gold_label']}\")\n",
    "    print(f\"\\tReason: {error['reason']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d8df7",
   "metadata": {},
   "source": [
    "factors: \n",
    "\n",
    "1. Lexical and Paraphrastic Variation\n",
    "\n",
    "2. Syntactic Complexity and Ambiguity\n",
    "\n",
    "3. World Knowledge and Commonsense Reasoning\n",
    "\n",
    "4. Vague Cases and Graded Entailment\n",
    "\n",
    "5. Hidden Assumptions and Event Coreference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ed5108",
   "metadata": {},
   "source": [
    "## Error Analysis table\n",
    "\n",
    "### Based on the analysis of 20 error samples from the ANLI baseline model, here are the key observations:\n",
    "\n",
    "| **#** | **Error Summary** | **Reason Summary** | **Factor** |\n",
    "|-------|-------------------|--------------------|------------|\n",
    "|   1   | Assumed writing from directing | Small detail added, many other details correct → confused model | Hidden Assumptions |\n",
    "|   2   | Secret sevice assignment | Not stated in text, model assumed cabinet role implies protection | World Knowledge |\n",
    "|   3   | Halley born outside UK | English nationality ≠ confirmed UK birth | World Knowledge |\n",
    "|   4   | Plant found all over the world | Native to North America, but global presence not mentioned | World Knowledge |\n",
    "|   5   | Film broadcast assumptions | Broadcasting not mentioned, the AI can’t assume | Hidden Assumptions |\n",
    "|   6   | Champion in 3rd edition | \"Defending\" implies prior win, the model didn’t infer that | Hidden Assumptions |\n",
    "|   7   | Armenian Film Festival | \tNo evidence of 2002 event, model over-inferred | Vague Cases |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
