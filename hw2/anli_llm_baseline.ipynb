{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"grok_key.ini\") \n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "## Implement the DSPy classifier program.\n",
    "class ANLIClassifier(dspy.Signature):\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    label: Literal['entailment', 'neutral', 'contradiction'] = dspy.OutputField()\n",
    "\n",
    "classier = dspy.Predict(ANLIClassifier)\n",
    "\n",
    "def classify(premise, hypothesis):\n",
    "    return classier(premise=premise, hypothesis=hypothesis).label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7056e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the llm model on a dataset section\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_llm_on_section(section):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "\n",
    "    data = dataset[section]\n",
    "\n",
    "    for example in tqdm(data):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = classify(premise, hypothesis)\n",
    "\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'pred_label': prediction,\n",
    "            'gold_label': label_names[example['label']]\n",
    "        })\n",
    "\n",
    "    # Calculate metrics\n",
    "    predictions = [result['pred_label'] for result in results]\n",
    "    references = [result['gold_label'] for result in results]\n",
    "\n",
    "    label_to_int = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "    predictions_int = [label_to_int[pred] for pred in predictions]\n",
    "    references_int = [label_to_int[ref] for ref in references]\n",
    "\n",
    "    accuracy_score = accuracy.compute(predictions=predictions_int, references=references_int)['accuracy']\n",
    "    f1_score = f1.compute(predictions=predictions_int, references=references_int, average='macro')['f1']\n",
    "    precision_score = precision.compute(predictions=predictions_int, references=references_int, average='macro')['precision']\n",
    "    recall_score = recall.compute(predictions=predictions_int, references=references_int, average='macro')['recall']\n",
    "\n",
    "    print(f\"Results for section {section}:\")\n",
    "    print(f\"\\tAccuracy: {accuracy_score:.3f}\")\n",
    "    print(f\"\\tF1: {f1_score:.3f}\")\n",
    "    print(f\"\\tPrecision: {precision_score:.3f}\")\n",
    "    print(f\"\\tRecall: {recall_score:.3f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f44b8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:04:32<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for section test_r1:\n",
      "\tAccuracy: 0.824\n",
      "\tF1: 0.826\n",
      "\tPrecision: 0.839\n",
      "\tRecall: 0.824\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:14:55<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for section test_r2:\n",
      "\tAccuracy: 0.759\n",
      "\tF1: 0.761\n",
      "\tPrecision: 0.786\n",
      "\tRecall: 0.759\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [1:15:36<00:00,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for section test_r3:\n",
      "\tAccuracy: 0.691\n",
      "\tF1: 0.694\n",
      "\tPrecision: 0.744\n",
      "\tRecall: 0.691\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on each test section\n",
    "llm_result = {}\n",
    "sections = ['test_r1', 'test_r2', 'test_r3']\n",
    "\n",
    "for section in sections:\n",
    "    llm_result[section] = evaluate_llm_on_section(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7345a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DeBERTa model functions\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "deberta_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "deberta_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def evaluate_deberta(premise, hypothesis):\n",
    "    input_data = deberta_tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    output = deberta_model(input_data[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    prediction_dict = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    \n",
    "    if prediction_dict[\"entailment\"] > prediction_dict[\"contradiction\"] and prediction_dict[\"entailment\"] > prediction_dict[\"neutral\"]:\n",
    "        return \"entailment\"\n",
    "    elif prediction_dict[\"contradiction\"] > prediction_dict[\"entailment\"]:\n",
    "        return \"contradiction\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22933482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the DeBERTa model on a dataset section\n",
    "def evaluate_deberta_on_section(section):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "\n",
    "    data = dataset[section]\n",
    "\n",
    "    for example in tqdm(data):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = evaluate_deberta(premise, hypothesis)\n",
    "\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'pred_label': prediction,\n",
    "            'gold_label': label_names[example['label']]\n",
    "        })\n",
    "\n",
    "    # Calculate metrics\n",
    "    predictions = [result['pred_label'] for result in results]\n",
    "    references = [result['gold_label'] for result in results]\n",
    "\n",
    "    label_to_int = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "    predictions_int = [label_to_int[pred] for pred in predictions]\n",
    "    references_int = [label_to_int[ref] for ref in references]\n",
    "\n",
    "    accuracy_score = accuracy.compute(predictions=predictions_int, references=references_int)['accuracy']\n",
    "    f1_score = f1.compute(predictions=predictions_int, references=references_int, average='macro')['f1']\n",
    "    precision_score = precision.compute(predictions=predictions_int, references=references_int, average='macro')['precision']\n",
    "    recall_score = recall.compute(predictions=predictions_int, references=references_int, average='macro')['recall']\n",
    "\n",
    "    print(f\"Results for section {section}:\")\n",
    "    print(f\"\\tAccuracy: {accuracy_score:.3f}\")\n",
    "    print(f\"\\tF1: {f1_score:.3f}\")\n",
    "    print(f\"\\tPrecision: {precision_score:.3f}\")\n",
    "    print(f\"\\tRecall: {recall_score:.3f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8881c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:15<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for section test_r1:\n",
      "\tAccuracy: 0.700\n",
      "\tF1: 0.677\n",
      "\tPrecision: 0.713\n",
      "\tRecall: 0.694\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:16<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for section test_r2:\n",
      "\tAccuracy: 0.480\n",
      "\tF1: 0.464\n",
      "\tPrecision: 0.502\n",
      "\tRecall: 0.470\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for section test_r3:\n",
      "\tAccuracy: 0.460\n",
      "\tF1: 0.248\n",
      "\tPrecision: 0.354\n",
      "\tRecall: 0.267\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\hadar\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "debrata_result = {}\n",
    "for section in sections:\n",
    "    debrata_result[section] = evaluate_deberta_on_section(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05259b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the LLM results with DeBERTa\n",
    "def compare_results(llm_results, deberta_results):\n",
    "    results = {}\n",
    "    sections = ['test_r1', 'test_r2', 'test_r3']\n",
    "    \n",
    "    for section in sections:\n",
    "        both_correct = 0\n",
    "        llm_correct = 0\n",
    "        deberta_correct = 0\n",
    "        both_incorrect = 0\n",
    "        total = 0\n",
    "\n",
    "        llm_section_results = llm_results[section]\n",
    "        deberta_section_results = deberta_results[section]\n",
    "        \n",
    "        for llm_result, deberta_result in zip(llm_section_results, deberta_section_results):\n",
    "            total += 1\n",
    "\n",
    "            llm_pred = llm_result['pred_label']\n",
    "            deberta_pred = deberta_result['pred_label']\n",
    "            gold = llm_result['gold_label']\n",
    "\n",
    "            if llm_pred == gold and deberta_pred == gold:\n",
    "                both_correct += 1\n",
    "            elif llm_pred == gold and deberta_pred != gold:\n",
    "                llm_correct += 1\n",
    "            elif llm_pred != gold and deberta_pred == gold:\n",
    "                deberta_correct += 1\n",
    "            else:\n",
    "                both_incorrect += 1\n",
    "\n",
    "        results[section] = {\n",
    "            'both_correct': both_correct,\n",
    "            'llm_correct': llm_correct,\n",
    "            'deberta_correct': deberta_correct,\n",
    "            'both_incorrect': both_incorrect,\n",
    "            'total': total\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "comparison = compare_results(llm_result, debrata_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37ae623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: test_r1\n",
      "\tBoth Correct: 31 (62.00%)\n",
      "\tLLM Correct: 10 (20.00%)\n",
      "\tDeBERTa Correct: 4 (8.00%)\n",
      "\tBoth Incorrect: 5 (10.00%)\n",
      "--------------------------------------------------\n",
      "Section: test_r2\n",
      "\tBoth Correct: 19 (38.00%)\n",
      "\tLLM Correct: 22 (44.00%)\n",
      "\tDeBERTa Correct: 5 (10.00%)\n",
      "\tBoth Incorrect: 4 (8.00%)\n",
      "--------------------------------------------------\n",
      "Section: test_r3\n",
      "\tBoth Correct: 14 (28.00%)\n",
      "\tLLM Correct: 11 (22.00%)\n",
      "\tDeBERTa Correct: 9 (18.00%)\n",
      "\tBoth Incorrect: 16 (32.00%)\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Overall Results:\n",
      "\tBoth Correct: 64 (42.67%)\n",
      "\tLLM Correct: 43 (28.67%)\n",
      "\tDeBERTa Correct: 18 (12.00%)\n",
      "\tBoth Incorrect: 25 (16.67%)\n"
     ]
    }
   ],
   "source": [
    "def compare_all_sections(comparison):\n",
    "    for section in sections:\n",
    "        result = comparison[section]\n",
    "        print(f\"Section: {section}\")\n",
    "        print(f\"\\tBoth Correct: {result['both_correct']} ({result['both_correct'] / result['total']:.2%})\")\n",
    "        print(f\"\\tLLM Correct: {result['llm_correct']} ({result['llm_correct'] / result['total']:.2%})\")\n",
    "        print(f\"\\tDeBERTa Correct: {result['deberta_correct']} ({result['deberta_correct'] / result['total']:.2%})\")\n",
    "        print(f\"\\tBoth Incorrect: {result['both_incorrect']} ({result['both_incorrect'] / result['total']:.2%})\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Overall Results:\")\n",
    "    total_both_correct = sum(result['both_correct'] for result in comparison.values())  \n",
    "    total_llm_correct = sum(result['llm_correct'] for result in comparison.values())\n",
    "    total_deberta_correct = sum(result['deberta_correct'] for result in comparison.values())\n",
    "    total_both_incorrect = sum(result['both_incorrect'] for result in comparison.values())\n",
    "    total = sum(result['total'] for result in comparison.values())  \n",
    "    print(f\"\\tBoth Correct: {total_both_correct} ({total_both_correct / total:.2%})\")\n",
    "    print(f\"\\tLLM Correct: {total_llm_correct} ({total_llm_correct / total:.2%})\")\n",
    "    print(f\"\\tDeBERTa Correct: {total_deberta_correct} ({total_deberta_correct / total:.2%})\")\n",
    "    print(f\"\\tBoth Incorrect: {total_both_incorrect} ({total_both_incorrect / total:.2%})\")\n",
    "\n",
    "compare_all_sections(comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
