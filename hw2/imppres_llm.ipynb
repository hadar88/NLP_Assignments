{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres with LLM\n",
    "\n",
    "You have to implement in this notebook a better ImpPres classifier using an LLM.\n",
    "This classifier must be implemented using DSPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"grok_key.ini\") \n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "\n",
    "## Implement the DSPy classifier program.\n",
    "class ParadigmClassifier(dspy.Signature):\n",
    "    pairs: str = dspy.InputField(desc=\"All premise-hypothesis pairs, numbered and separated by |\")\n",
    "    predictions: List[Literal['entailment', 'neutral', 'contradiction']] = dspy.OutputField(desc=\"List of predictions for each pair\")\n",
    "\n",
    "classifier = dspy.Predict(ParadigmClassifier)\n",
    "\n",
    "def classify(paradigm_pairs):\n",
    "    pairs = []\n",
    "    for i, pair in enumerate(paradigm_pairs):\n",
    "        s = f\"{i + 1}. Premise: {pair['premise']}, Hypothesis: {pair['hypothesis']}\"\n",
    "        pairs.append(s)\n",
    "\n",
    "    pairs_str = \" | \".join(pairs)\n",
    "    results = classifier(pairs=pairs_str)\n",
    "\n",
    "    return results.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ImpPres Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0438789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for section: presupposition_all_n_presupposition\n",
      "Loading dataset for section: presupposition_both_presupposition\n",
      "Loading dataset for section: presupposition_change_of_state\n",
      "Loading dataset for section: presupposition_cleft_existence\n",
      "Loading dataset for section: presupposition_cleft_uniqueness\n",
      "Loading dataset for section: presupposition_only_presupposition\n",
      "Loading dataset for section: presupposition_possessed_definites_existence\n",
      "Loading dataset for section: presupposition_possessed_definites_uniqueness\n",
      "Loading dataset for section: presupposition_question_presupposition\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sections = ['presupposition_all_n_presupposition', \n",
    "            'presupposition_both_presupposition', \n",
    "            'presupposition_change_of_state', \n",
    "            'presupposition_cleft_existence', \n",
    "            'presupposition_cleft_uniqueness', \n",
    "            'presupposition_only_presupposition', \n",
    "            'presupposition_possessed_definites_existence', \n",
    "            'presupposition_possessed_definites_uniqueness', \n",
    "            'presupposition_question_presupposition']\n",
    "\n",
    "dataset = {}\n",
    "for section in sections:\n",
    "    print(f\"Loading dataset for section: {section}\")\n",
    "    dataset[section] = load_dataset(\"facebook/imppres\", section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "613fcaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def evaluate_paradigm_section(dataset):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    paradigm_scores = []\n",
    "\n",
    "    # Make paradigm groups\n",
    "    paradigms = []\n",
    "    for i in range(0, len(dataset), 19):\n",
    "        if i + 19 <= len(dataset):\n",
    "            paradigm = []\n",
    "            for j in range(19):\n",
    "                example = dataset[i + j]\n",
    "                paradigm.append({\n",
    "                    'premise': example['premise'],\n",
    "                    'hypothesis': example['hypothesis'],\n",
    "                    'gold_label': label_names[example['gold_label']]\n",
    "                })\n",
    "            random.shuffle(paradigm)\n",
    "            paradigms.append(paradigm)\n",
    "\n",
    "    for paradigm in tqdm(paradigms):\n",
    "        pairs = [{'premise': pair['premise'], 'hypothesis': pair['hypothesis']} for pair in paradigm]\n",
    "        predictions = classify(pairs)\n",
    "\n",
    "        # Calculate accuracy score\n",
    "        correct = 0\n",
    "        for pred, gold in zip(predictions, paradigm):\n",
    "            if pred == gold['gold_label']:\n",
    "                correct += 1\n",
    "        accuracy = correct / len(predictions)\n",
    "    \n",
    "        # Calculate consistency score - the proportion of the most common prediction\n",
    "        pred_counts = defaultdict(int)\n",
    "        for pred in predictions:\n",
    "            pred_counts[pred] += 1\n",
    "        consistency = max(pred_counts.values()) / len(predictions)\n",
    "\n",
    "        combined_score = (accuracy + consistency) / 2\n",
    "        paradigm_scores.append({\n",
    "            'consistency': consistency,\n",
    "            'combined': combined_score\n",
    "        })\n",
    "\n",
    "        paradigm_results = []\n",
    "        for pred, gold in zip(predictions, paradigm):\n",
    "            paradigm_results.append({\n",
    "                'pred_label': pred,\n",
    "                'gold_label': gold['gold_label']\n",
    "            })  \n",
    "        results.append(paradigm_results)\n",
    "\n",
    "    return results, paradigm_scores                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955c1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_all_n_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [15:40<00:00,  9.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_both_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [16:20<00:00,  9.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_change_of_state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [15:21<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section: presupposition_cleft_existence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [10:06<04:57,  9.01s/it]"
     ]
    }
   ],
   "source": [
    "for section in sections:\n",
    "    print(f\"Working on section: {section}\")\n",
    "    sec = section[15:]\n",
    "    data = dataset[section][sec]\n",
    "\n",
    "    results, paradigm_scores = evaluate_paradigm_section(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "results_table = []\n",
    "combined_scores = []\n",
    "transformation_results = []\n",
    "\n",
    "for section in sections:\n",
    "    print(f\"Working on section: {section}\")\n",
    "    sec = section[15:]\n",
    "    data = dataset[section][sec]\n",
    "\n",
    "    results, paradigm_scores = evaluate_paradigm_section(data)\n",
    "\n",
    "    # Calculate metrics\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for paradigm in results:\n",
    "        for result in paradigm:\n",
    "            precisions.append(result['pred_label'])\n",
    "            references.append(result['gold_label'])\n",
    "\n",
    "    label_to_int = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "    predictions_int = [label_to_int[pred] for pred in predictions]\n",
    "    references_int = [label_to_int[ref] for ref in references]\n",
    "\n",
    "    accuracy_score = accuracy.compute(predictions=predictions_int, references=references_int)['accuracy']\n",
    "    f1_score = f1.compute(predictions=predictions_int, references=references_int, average='macro')['f1']\n",
    "    precision_score = precision.compute(predictions=predictions_int, references=references_int, average='macro', zero_division=0)['precision']\n",
    "    recall_score = recall.compute(predictions=predictions_int, references=references_int, average='macro', zero_division=0)['recall']\n",
    "\n",
    "    accuracies.append(accuracy_score)\n",
    "    precisions.append(precision_score)\n",
    "    recalls.append(recall_score)\n",
    "    f1s.append(f1_score)\n",
    "\n",
    "    consistency = sum([p['consistency'] for p in paradigm_scores]) / len(paradigm_scores)\n",
    "    combined_score = sum([p['combined'] for p in paradigm_scores]) / len(paradigm_scores)\n",
    "\n",
    "    # Calculate metrics for each transformation type\n",
    "    transformation_metrics = []\n",
    "    for type in range(19):\n",
    "        type_predictions = []\n",
    "        type_references = []\n",
    "\n",
    "        for paradigm in results:\n",
    "            if type < len(paradigm):\n",
    "                type_predictions.append(paradigm[type]['pred_label'])\n",
    "                type_references.append(paradigm[type]['gold_label'])\n",
    "\n",
    "        type_predictions_int = [label_to_int[pred] for pred in type_predictions]\n",
    "        type_references_int = [label_to_int[ref] for ref in type_references]\n",
    "\n",
    "        type_accuracy = accuracy.compute(predictions=type_predictions_int, references=type_references_int)['accuracy']\n",
    "        type_f1 = f1.compute(predictions=type_predictions_int, references=type_references_int, average='macro')['f1']\n",
    "        type_precision = precision.compute(predictions=type_predictions_int, references=type_references_int, average='macro', zero_division=0)['precision']\n",
    "        type_recall = recall.compute(predictions=type_predictions_int, references=type_references_int, average='macro', zero_division=0)['recall']\n",
    "\n",
    "        transformation_metrics.append({\n",
    "            'Accuracy': f\"{type_accuracy:.2f}\",\n",
    "            'Precision': f\"{type_precision:.2f}\",\n",
    "            'Recall': f\"{type_recall:.2f}\",\n",
    "            'F1': f\"{type_f1:.2f}\"\n",
    "        })\n",
    "\n",
    "    transformation_results.append(transformation_metrics)\n",
    "\n",
    "    results_table.append({\n",
    "        'Section': section,\n",
    "        'Accuracy': f\"{accuracy_score:.2f}\",\n",
    "        'Precision': f\"{precision_score:.2f}\",\n",
    "        'Recall': f\"{recall_score:.2f}\",\n",
    "        'F1': f\"{f1_score:.2f}\",\n",
    "        'Consistency': f\"{consistency:.2f}\",\n",
    "        'Combined': f\"{combined_score:.2f}\"\n",
    "    })\n",
    "\n",
    "    break\n",
    "\n",
    "# Calculate overall metrics\n",
    "accuracy_all = sum(accuracies) / len(accuracies)\n",
    "precision_all = sum(precisions) / len(precisions)\n",
    "recall_all = sum(recalls) / len(recalls)\n",
    "f1_all = sum(f1s) / len(f1s)\n",
    "consistency_all = sum([r['consistency'] for r in results_table]) / len(results_table)\n",
    "combined_all = sum([r['combined'] for r in results_table]) / len(results_table)\n",
    "\n",
    "results_table.append({\n",
    "    'Section': 'Overall',\n",
    "    'Accuracy': f\"{accuracy_all:.2f}\",\n",
    "    'Precision': f\"{precision_all:.2f}\",\n",
    "    'Recall': f\"{recall_all:.2f}\",\n",
    "    'F1': f\"{f1_all:.2f}\",\n",
    "    'Consistency': f\"{consistency_all:.2f}\",\n",
    "    'Combined': f\"{combined_all:.2f}\"\n",
    "})\n",
    "\n",
    "# Display section results as a table\n",
    "print(\"Section Results:\")\n",
    "results_df = pd.DataFrame(results_table)\n",
    "styled_df = results_df.style.set_properties(**{'text-align': 'center'})\n",
    "styled_df = styled_df.set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n",
    "display(styled_df)\n",
    "\n",
    "# Display transformation results as a table\n",
    "transformations = []\n",
    "for result in transformation_results: # 100\n",
    "    transformation = {\n",
    "        'Accuracy': 0,\n",
    "        'Precision': 0,\n",
    "        'Recall': 0,\n",
    "        'F1': 0\n",
    "    }\n",
    "    for metrics in results: # 19\n",
    "        transformation['Accuracy'] += metrics['Accuracy']\n",
    "        transformation['Precision'] += metrics['Precision']\n",
    "        transformation['Recall'] += metrics['Recall']\n",
    "        transformation['F1'] += metrics['F1']\n",
    "    transformation['Accuracy'] /= len(result)\n",
    "    transformation['Precision'] /= len(result)\n",
    "    transformation['Recall'] /= len(result)\n",
    "    transformation['F1'] /= len(result)\n",
    "    transformations.append(transformation)\n",
    "\n",
    "print(\"Transformation Results:\")\n",
    "transformation_df = pd.DataFrame(transformations)\n",
    "styled_df = transformation_df.style.set_properties(**{'text-align': 'center'})\n",
    "styled_df = styled_df.set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc2249",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
