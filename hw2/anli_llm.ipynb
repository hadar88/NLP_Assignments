{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI with LLM\n",
    "\n",
    "You have to implement in this notebook a better ANLI classifier using an LLM.\n",
    "This classifier must be implemented using DSPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"grok_key.ini\")\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# Load the model for similarity scoring\n",
    "similarity_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Joint prompt strategy\n",
    "class ANLIJointCoT(dspy.Signature):\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    explanation: str = dspy.OutputField(desc=\"Explanation of the label\")\n",
    "    label: Literal['entailment', 'neutral', 'contradiction'] = dspy.OutputField()\n",
    "\n",
    "# Pipeline strategy\n",
    "class ANLICOTExplanation(dspy.Signature):\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    explanation: str = dspy.OutputField(desc=\"Explanation of the label\")\n",
    "\n",
    "class ANLILabel(dspy.Signature):\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    explanation: str = dspy.InputField()\n",
    "    label: Literal['entailment', 'neutral', 'contradiction'] = dspy.OutputField()\n",
    "\n",
    "joint_cot = dspy.Predict(ANLIJointCoT)\n",
    "explain_cot = dspy.Predict(ANLICOTExplanation)\n",
    "label_cot = dspy.Predict(ANLILabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11b97bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_similarity(premise, hypothesis, human_explanation, predicted_explanation):\n",
    "    results = {}\n",
    "\n",
    "    premis_hypothesis = f\"Premise: {premise}\\nHypothesis: {hypothesis}\"\n",
    "    passages = [premis_hypothesis, human_explanation, predicted_explanation]\n",
    "\n",
    "    embeddings = similarity_model.encode(passages)\n",
    "    similarities = similarity_model.similarity(embeddings, embeddings)\n",
    "\n",
    "    results[\"premise_hypothesis_vs_human\"] = similarities[0][1].item()\n",
    "    results[\"premise_hypothesis_vs_predicted\"] = similarities[0][2].item()\n",
    "    results[\"human_vs_predicted\"] = similarities[1][2].item()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0507882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [01:00<04:30, 10.39s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = dataset[\"dev_r3\"].select(range(50))\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "\n",
    "joint_results = []\n",
    "pipeline_results = []\n",
    "\n",
    "for example in tqdm(data):\n",
    "    premise = example['premise']\n",
    "    hypothesis = example['hypothesis']\n",
    "    human_explanation = example['reason']\n",
    "    gold_label = label_names[example['label']]\n",
    "\n",
    "    # Joint method    \n",
    "    joint_output = joint_cot(premise=premise, hypothesis=hypothesis)\n",
    "    joint_explanation = joint_output.explanation\n",
    "    joint_label = joint_output.label\n",
    "\n",
    "    # Pipeline method\n",
    "    pipeline_explanation = explain_cot(premise=premise, hypothesis=hypothesis).explanation\n",
    "    pipeline_label = label_cot(premise=premise, hypothesis=hypothesis, explanation=pipeline_explanation).label\n",
    "\n",
    "    # Similarities\n",
    "    joint_similarities = rank_similarity(premise, hypothesis, human_explanation, joint_explanation)\n",
    "    pipeline_similarities = rank_similarity(premise, hypothesis, human_explanation, pipeline_explanation)\n",
    "\n",
    "    joint_results.append({\n",
    "        \"gold_label\": gold_label,\n",
    "        \"predicted_label\": joint_label,\n",
    "        \"premise_hypothesis_vs_human\": joint_similarities[\"premise_hypothesis_vs_human\"],\n",
    "        \"premise_hypothesis_vs_predicted\": joint_similarities[\"premise_hypothesis_vs_predicted\"],\n",
    "        \"human_vs_predicted\": joint_similarities[\"human_vs_predicted\"]\n",
    "    })\n",
    "\n",
    "    pipeline_results.append({\n",
    "        \"gold_label\": gold_label,\n",
    "        \"predicted_label\": pipeline_label,\n",
    "        \"premise_hypothesis_vs_human\": pipeline_similarities[\"premise_hypothesis_vs_human\"],\n",
    "        \"premise_hypothesis_vs_predicted\": pipeline_similarities[\"premise_hypothesis_vs_predicted\"],\n",
    "        \"human_vs_predicted\": pipeline_similarities[\"human_vs_predicted\"]\n",
    "    })  \n",
    "\n",
    "# Calculate metrics\n",
    "references = [result['gold_label'] for result in joint_results]\n",
    "\n",
    "joint_predictions = [result['predicted_label'] for result in joint_results]\n",
    "pipeline_predictions = [result['predicted_label'] for result in pipeline_results]\n",
    "\n",
    "label_to_int = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "references_int = [label_to_int[ref] for ref in references]\n",
    "joint_predictions_int = [label_to_int[pred] for pred in joint_predictions]\n",
    "pipeline_predictions_int = [label_to_int[pred] for pred in pipeline_predictions]\n",
    "\n",
    "joint_accuracy_score = accuracy.compute(predictions=joint_predictions_int, references=references_int)['accuracy']\n",
    "joint_f1_score = f1.compute(predictions=joint_predictions_int, references=references_int, average='macro')['f1']\n",
    "joint_precision_score = precision.compute(predictions=joint_predictions_int, references=references_int, average='macro')['precision']\n",
    "joint_recall_score = recall.compute(predictions=joint_predictions_int, references=references_int, average='macro')['recall']\n",
    "\n",
    "pipeline_accuracy_score = accuracy.compute(predictions=pipeline_predictions_int, references=references_int)['accuracy']\n",
    "pipeline_f1_score = f1.compute(predictions=pipeline_predictions_int, references=references_int, average='macro')['f1']\n",
    "pipeline_precision_score = precision.compute(predictions=pipeline_predictions_int, references=references_int, average='macro')['precision']\n",
    "pipeline_recall_score = recall.compute(predictions=pipeline_predictions_int, references=references_int, average='macro')['recall']\n",
    "\n",
    "print(\"Joint Method Results:\")\n",
    "print(f\"\\tAccuracy: {joint_accuracy_score:.3f}\")\n",
    "print(f\"\\tF1: {joint_f1_score:.3f}\")\n",
    "print(f\"\\tPrecision: {joint_precision_score:.3f}\")\n",
    "print(f\"\\tRecall: {joint_recall_score:.3f}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Pipeline Method Results:\")\n",
    "print(f\"\\tAccuracy: {pipeline_accuracy_score:.3f}\")\n",
    "print(f\"\\tF1: {pipeline_f1_score:.3f}\")\n",
    "print(f\"\\tPrecision: {pipeline_precision_score:.3f}\")\n",
    "print(f\"\\tRecall: {pipeline_recall_score:.3f}\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "print(\"=\"*50)\n",
    "\n",
    "# analyze similarities\n",
    "joint_premise_hypothesis_vs_human = [result['premise_hypothesis_vs_human'] for result in joint_results]\n",
    "joint_premise_hypothesis_vs_predicted = [result['premise_hypothesis_vs_predicted'] for result in joint_results]\n",
    "joint_human_vs_predicted = [result['human_vs_predicted'] for result in joint_results]\n",
    "pipeline_premise_hypothesis_vs_human = [result['premise_hypothesis_vs_human'] for result in pipeline_results]\n",
    "pipeline_premise_hypothesis_vs_predicted = [result['premise_hypothesis_vs_predicted'] for result in pipeline_results]\n",
    "pipeline_human_vs_predicted = [result['human_vs_predicted'] for result in pipeline_results] \n",
    "print(\"Joint Method Similarities:\")\n",
    "print(f\"\\tPremise-Hypothesis vs Human: {sum(joint_premise_hypothesis_vs_human) / len(joint_premise_hypothesis_vs_human):.3f}\")\n",
    "print(f\"\\tPremise-Hypothesis vs Predicted: {sum(joint_premise_hypothesis_vs_predicted) / len(joint_premise_hypothesis_vs_predicted):.3f}\")\n",
    "print(f\"\\tHuman vs Predicted: {sum(joint_human_vs_predicted) / len(joint_human_vs_predicted):.3f}\")\n",
    "print(\"-\" * 50) \n",
    "print(\"Pipeline Method Similarities:\")\n",
    "print(f\"\\tPremise-Hypothesis vs Human: {sum(pipeline_premise_hypothesis_vs_human) / len(pipeline_premise_hypothesis_vs_human):.3f}\")\n",
    "print(f\"\\tPremise-Hypothesis vs Predicted: {sum(pipeline_premise_hypothesis_vs_predicted) / len(pipeline_premise_hypothesis_vs_predicted):.3f}\")\n",
    "print(f\"\\tHuman vs Predicted: {sum(pipeline_human_vs_predicted) / len(pipeline_human_vs_predicted):.3f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"=\"*50)\n",
    "print(\"SUMMARY COMPARISON:\")\n",
    "print(f\"Better Classification: {'Joint' if joint_accuracy_score > pipeline_accuracy_score else 'Pipeline'} ({joint_accuracy_score:.3f} vs {pipeline_accuracy_score:.3f})\")\n",
    "print(f\"Better Explanation Relevance: {'Pipeline' if sum(pipeline_premise_hypothesis_vs_predicted) > sum(joint_premise_hypothesis_vs_predicted) else 'Joint'} ({sum(pipeline_premise_hypothesis_vs_predicted)/len(pipeline_premise_hypothesis_vs_predicted):.3f} vs {sum(joint_premise_hypothesis_vs_predicted)/len(joint_premise_hypothesis_vs_predicted):.3f})\")\n",
    "print(f\"Better Human Similarity: {'Pipeline' if sum(pipeline_human_vs_predicted) > sum(joint_human_vs_predicted) else 'Joint'} ({sum(pipeline_human_vs_predicted)/len(pipeline_human_vs_predicted):.3f} vs {sum(joint_human_vs_predicted)/len(joint_human_vs_predicted):.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
