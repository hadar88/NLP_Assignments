{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b8c418",
   "metadata": {},
   "source": [
    "# Exploring ANLI Dataset\n",
    "\n",
    "The https://huggingface.co/datasets/facebook/anli dataset was introduced in \"Adversarial NLI: A New Benchmark for Natural Language Understanding\" (Yixin et al, ACL 2020) https://aclanthology.org/2020.acl-main.441/ as a hard natural language entailment dataset.\n",
    "\n",
    "It was created by collecting pairs (premise, hypothesis) that existing models could not properly categorize as entailment/contradiction or neutral.  For such cases, human annotators were asked to provide an explanation for their tagging decision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0227b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "653292e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2406d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uid': '4aae63a8-fcf7-406c-a2f3-50c31c5934a9',\n",
       " 'premise': 'Ernest Jones is a British jeweller and watchmaker. Established in 1949, its first store was opened in Oxford Street, London. Ernest Jones specialises in diamonds and watches, stocking brands such as Gucci and Emporio Armani. Ernest Jones is part of the Signet Jewelers group.',\n",
       " 'hypothesis': 'The first Ernest Jones store was opened on the continent of Europe.',\n",
       " 'label': 0,\n",
       " 'reason': \"The first store was opened in London, which is in Europe. It may have been difficult for the system because continents weren't mentioned.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test_r1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f330b353",
   "metadata": {},
   "source": [
    "In https://aclanthology.org/2023.findings-eacl.162/ (Kavumba et al, EACL 2023), experiments demonstrate that when LLMs are prompted to classify a pair (premise, hypothesis) as entailment/contradiction/neutral, they perform better if the prompt also requires an explanation to justify the selected label.  \n",
    "\n",
    "In order for the explanation to be helpful, though, it must be a \"relevant\" explanation, that is, a sentence that is related semantically to the premise and the hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29774235",
   "metadata": {},
   "source": [
    "Your mission in this question is to reproduce these empirical observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c5fc6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
